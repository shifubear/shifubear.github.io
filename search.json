[
  {
    "objectID": "research/stoqma.html",
    "href": "research/stoqma.html",
    "title": "The Living StoqMA Page",
    "section": "",
    "text": "The introduction of the theory of quantum computation has added many new questions in complexity which may have not risen naturally in the classical realm. Many who study quantum computing may encounter the classes without reference to their source, and be quite confused to why such definitions are necessary or even arose in the first place. However, when examined more closely, we find that the sources of these classes are quite natural questions to ask about quantum computation, and known results and hypotheses give new ways to understand these questions. StoqMA is one such class, whose definition as a complexity class looks quite nonsensical at first, but has led to answering some interesting questions about quantum computing and complexity theory.\n\nDefinition\nThe class StoqMA is defined by the use of a stoquastic verifier. A stoquastic verifier is a verifier which can be modeled as a classical reversible circuit, which is a essentially a quantum circuit that only uses X, CNOT, and Toffoli gates. The verifier takes as input a classical string \\(x\\) along with a quantum witness which are passed to the circuit.\nA stoquastic verifier is a tuple \\(V = (n, n_w, n_0, n_+, U)\\), where \\(n\\) is the number of input bits, \\(n_w\\) the number of input witness qubits, \\(n_0\\) the number of input auxiliary qubits \\(\\ket{0}\\), \\(n_+\\) the number of input auxiliary qubits \\(\\ket{+}\\) and \\(U\\) is a quantum circuit on \\(n + n_w + n_0 + n_+\\) qubits with NOT, CNOT, and Toffoli gates (classical reversible circuit). The acceptance probability of a stoquastic verifier \\(V\\) on input string \\(x \\in \\{0, 1\\}^n\\) and \\(n_w\\)-qubit witness state \\(\\ket{\\psi}\\) is defined as \\(\\text{Pr}[V \\text{ accepts } (x, \\ket{\\psi})] = \\bra{\\psi_{in}}U^\\dagger \\Pi_{out} U \\ket{\\psi_{in}}\\). Here, \\(\\ket{\\psi_{in}} = \\ket{x} \\otimes \\ket{\\psi} \\otimes \\ket{0}^{\\otimes n_0} \\otimes \\ket{+}^{\\otimes n_+}\\) is the initial state and \\(\\Pi_{out} = \\ket{+}\\bra{+}_1 \\otimes I_{else}\\) projects the first qubit onto the state \\(\\ket{+}\\).\nA promise problem \\(L = (L_{yes}, L_{no}) \\in {StoqMA}(a,b)\\) iff there exists a uniform family of stoquastic verifiers, such that for any fixed number of input bits \\(n\\) the corresponding verifier \\(V\\) uses at most \\(p_1(n)\\) qubits, \\(p_2(n)\\) gates where both \\(p_1\\) and \\(p_2\\) are polynomials; and obeys the following: - Completeness: If \\(x \\in L_{yes}\\), then there is some \\(\\ket{\\psi}\\) s.t. \\(\\text{Pr}[V \\text{ accepts } (x, \\ket{\\psi})] \\geq a\\); - Soundness: If \\(x \\in L_{no}\\), then for any \\(\\ket{\\psi}\\), \\(\\text{Pr}[V \\text{ accepts } (x, \\ket{\\psi})] \\leq b\\)."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "CS 166: Intro to Quantum Computing (Winter 2024)"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Currently I’m interested in questions regarding state and unitary synthesis. In the classical setting, computer scientists like to study decision problems, which are questions which have a yes or no question (e.g. can I choose numbers from this list so they sum to 0? can I solve this maze? Is 2384973874927 a prime number?), since we have a good idea of how to use the answer to the decision problem to find a solution we are interested in. Quantumly, this is much more difficult. The structure of quantum states is much more complex, and it’s not clear how to go from the answer of a decision problem to finding a certain state that we want (My advisor Sandy wrote a paper on this). This inspired a new line of research asking a new question, how hard is it to synthesize an arbitrary state? It’s surprising that the language around this question is just being formalized, even though it seems like a very natural question to ask. An extension to this question that is also developing is asking how hard it is to implement some desired operations on quantum states.\nSome recent results have shown evidence that perhaps the hardness of quantum problems comes from the measurement problem. This problem can roughly be stated as finding the best way to measure a quantum state to extract the most information. The evidence of the hardness of this problem is highlighting what we are missing in classical complexity theory, and may be the way to generalize the problem setting in order to solve many unexplored problems."
  },
  {
    "objectID": "posts/21-08-14-cassava/index.html",
    "href": "posts/21-08-14-cassava/index.html",
    "title": "Cassava Leaf Detection",
    "section": "",
    "text": "Cassava is one of the largest providers of carbohydrates in Africa, due to its nutritional value and its ability to withstand harsh conditions. However, the crop is not immune to a variety of viral diseases which are the cause of a low crop yield for the 80 percent of sub-Saharan household farms that grow Cassava. Existing methods of detecting viral diseases rely on a small group of experts manually examining each plant to identify the type of disease. This is a very inefficient solution to this problem but works because most viral diseases have clear visually detectable symptoms. Because of this nature, solutions to this problem based on using image data are very well studied as shown in this survey.\nA competition was hosted on Kaggle to come up with techniques to classify Cassava diseases using pictures of the plants. Though we weren’t able to participate in the live version, my group decided this would be a fun problem to tackle as the final project for our machine learning class.\n\n\n\n\n    \n\n\nFigure 1: Example Images from the data set. In the top row, we have bacterial blight, brown streak disease, and green mottle from left to right. On the bottom row, we have mottle disease and a healthy Cassava plant from left to right.\n\n\nSince farmers primarily only have access to mobile devices, we wanted to create a model that was lightweight but still accurate enough to classify photos taken with close to 90% accuracy. To accomplish this, we used convolutional neural networks created through transfer learning of the MobileNetV2 neural network."
  },
  {
    "objectID": "posts/21-08-14-cassava/index.html#convolutional-neural-networks-and-transfer-learning",
    "href": "posts/21-08-14-cassava/index.html#convolutional-neural-networks-and-transfer-learning",
    "title": "Cassava Leaf Detection",
    "section": "Convolutional Neural Networks and Transfer Learning",
    "text": "Convolutional Neural Networks and Transfer Learning\nAfter outperforming all other known models in the 2012 ImageNet Large-Scale Visual Recognition Challenge, deep convolutional neural networks have been seen as a very successful tool in tackling image classification problems. Though powerful, a challenge with CNNs is that they require a huge training dataset and extensive computing power. There are many great resources on CNNs on the web, so I will spare the technical details in this post.\nTo get around this problem, transfer learning has been proposed as a way to take advantage of the quality of CNN models, while not requiring as much resources for training. In transfer learning, a CNN is first pre-trained on a different, much larger data set and later fine-tuned on the data set corresponding to the problem of interest. The transferability of such networks has been an active area of research, and we decided to try it out for this project ourselves.\nSince one of our goals was a lightweight model, we chose MobileNetV2 as our CNN to transfer from. MobileNetV2 is a model developed by a group from Google, that is optimized for mobile devices."
  },
  {
    "objectID": "posts/21-08-14-cassava/index.html#data-augmentation-and-class-balanced-cross-entropy-loss",
    "href": "posts/21-08-14-cassava/index.html#data-augmentation-and-class-balanced-cross-entropy-loss",
    "title": "Cassava Leaf Detection",
    "section": "Data Augmentation and Class-Balanced Cross-Entropy Loss",
    "text": "Data Augmentation and Class-Balanced Cross-Entropy Loss\nWith an architecture in place, we are on a great start, but there is another problem that we are aware of prior to starting the training process. One, we would like to have more training data, and second, the data is heavily skewed towards a single class.\nTo solve the first problem, we use a popular technique in image classification called data augmentation, where we artificially create extra training data by randomly applying a combination of rotations, flips, and scaling operations on the existing data.\nTo solve the second problem, we used the class-balanced cross-entropy loss function that introduces a weighting factor that is inversely proportional to the effective number of samples. The equation to select the effective number of samples is \\(E_{n_i} = (1 - \\beta)^{n_i} / (1 - \\beta)\\) where \\(n_i\\) is the number of samples in class \\(i\\) and \\(\\beta\\) is 0.9999."
  },
  {
    "objectID": "posts/23-01-17-art-ai/index.html",
    "href": "posts/23-01-17-art-ai/index.html",
    "title": "Art and AI: A new era for art?",
    "section": "",
    "text": "Last year, OpenAI announced the second version of their text-to-image deep learning model, Dall-E 2. If you haven’t seen it, I encourage you to explore the linked page and check out some of what this model is capable of doing. If you’re anything like me, you will be shocked and fascinated at the quality and range of the results the model can produce. I spoke with fellow computer scientists who were surprised by how soon a model of this caliber appeared. Many of us were expecting something like this to appear in the near future, but not so soon (especially considering the level the first Dall-E was at just one year ago).\nI’m hopeful that through maintaining (or creating) a deep connection between the artist and audience, art will not be lost to AI and if anything this is an opportunity for us to sharpen our understanding and ideals for art’s role in society.\nIt has always been true that it may take a while for someone to discover something new, but once it’s found others are able to learn the mechanics fairly quickly. A similar thing can be said about the technology behind Dall-E 2 (transformers), and we have seen many alternative services deployed throughout this year for generating images from text prompts.\nThough these can be fun to play around with, there have been many debates surrounding the implications and ethics of using this technology. A quick search for AI art on any social media platform will reveal a wide range of opinions on the matter.\nBefore sharing my thoughts, let’s take a moment to deconstruct the main critique that is brought up against AI art. The following quote from Lois van Baarle’s instagram post captures the essence of the frustrations I’ve seen online: “I wholeheartedly support the ongoing protest against AI art. Why? Because my artwork is included in the datasets used to train these image generators without my consent.” Many of these posts are accompanied by strong anti-AI sentiments and a sense of fear in the comments about the implications of this emerging technology. The core of this critique is against the way these models are being trained and deployed, with the objective being to ban or restrict the usage of these models all together.\nI sympathize with regulating the training data that these models have access to, but I also do not think that there is much we are able to do to stop nefarious players to keep training and publicizing models. There are many counterarguments against the critique above as well, such as saying that the way the models learn from the data is the same as how human artists learn and get inspired from the art they engage with. As a computer scientist, it’s hard to object to this argument knowing the way these models learn from data, and how many people in the field believe that the way humans learn can be boiled down to the mechanics of a neural network. Ultimately, this discussion misses the mark of what is important, and instead we should take this opportunity to think about what art means to us and the role we want it to have moving forward."
  },
  {
    "objectID": "posts/23-01-17-art-ai/index.html#my-thoughts",
    "href": "posts/23-01-17-art-ai/index.html#my-thoughts",
    "title": "Art and AI: A new era for art?",
    "section": "My Thoughts",
    "text": "My Thoughts\nAs a huge lover of the arts and a budding computer scientist, this battle has been a painful one to watch unfold. However, I am hopeful for the future of art and technology especially in light of these conversations. My thoughts on the matter are that technologies like this force us to rethink what art means to us, and what we want it to mean to us moving forward.\nA quick tour through the development of computer science will help motivate my point. Computer science studies algorithms, which are a set of instructions for solving different problems. When creating these algorithms, we are forced to think about small decisions we make subconsciously for different tasks. Talk to any new undergraduate in computer science and they will tell you that it takes a lot of thought to tell a computer how to find a number in a list, something you could ask a child to do with three words: “Where is 27?”.\nIn the process of formalizing this field, an interesting question was posed by Alan Turing. “Is it possible to create a machine such that a person wouldn’t be able to distinguish whether it is a human or a machine?” This is referred to as the Turing test, and was set up as a major milestone for the field of computer science. Many people agreed that when a machine is able to do this, we will be able to call it “intelligent”.\nWe proudly label ourselves as an “intelligent” species, but the advance of technology continually forces us to wrestle with what that word means. For example, another “intelligent” pursuit humans have been very proud of is the ability to play chess, at extremely high levels. This is why the victory of Deep Blue against Gary Kasparov spurred conversation from around the dinner table to academic conferences, leading to an uproar about the takeover of AI.\nThe Turing test can also be cleared by many large scale language models these days, as was demonstrated by a Google engineer claiming that an AI chatbot is sentient. It seems that AI is passing the Turing test for image creation too, as is evident by these articles: Reddit’s Art Subreddit Shuts Down After Mod Mistakes Real Artist for AI, A Professional Artist Spent 100 Hours Working On This Book Cover Image, Only To Be Accused Of Using AI.\nEven though these “feats of intelligence” are now child’s play for these models, we refuse to accept that they’ve become “intelligent”. Chess is still extremely popular (arguably more popular than it’s ever been) and we don’t get excited to share our crazy day to a chatbot. Instead, we realize that these milestones for intelligence didn’t really capture the essence of the word. We push the goal post further and, in this process refine our understanding of intelligence and its role in what it means to be human.\nI’ve learned that part of studying computer science is facing a stream of confrontations on our understanding of intelligence. The reason I keep studying it is because the more I learn, the more my appreciation for the complexity of the human experience increases. I feel a sense of awe similar to when I stare into the depths of the ocean. Each discovery adding a ray of light, forcing us to confront the reality that the bottom is much deeper than we thought.\nIn a similar way, I feel like we are now being confronted with the opportunity to think about what art is, and the role it plays in our human experience. For me, art is never just about the work itself, but is a bridge for me to connect with others in ways that we can’t in conversation. My experiences with art I love are amplified by the stories they carry both explicitly and implicitly. The world the artist was living in when they created the work and the world I’m experiencing, combined, generates a unique perception of the work that only I can really experience. Sure, AI can be trained on large databases of images and mix those in clever ways to create new works but, for me, it’s hard to imagine that this will replace the role art plays in our lives. I am confident that when we think deeply about what remains after AI rummages through the space of what we call art, we will come out with a deeper appreciation of what it means to us in the first place.\nAt the end of the day, I may practice against a chess bot, but it’s so that I can play a great game with my friend the next time we’re sitting over a chess board. Chatbots have their useful moments, but what we remember from them are when they fail and we all laugh at the screenshot together. We won’t lose art if we take the time to slow down enough to share and listen to each others’ stories."
  },
  {
    "objectID": "posts/23-01-17-art-ai/index.html#a-new-era",
    "href": "posts/23-01-17-art-ai/index.html#a-new-era",
    "title": "Art and AI: A new era for art?",
    "section": "A New Era",
    "text": "A New Era\nStill, I do think we are at a moment in art history where a huge shift will occur, both for the artist and the audience. This isn’t only because of AI art, but other technologies like social media, video games and streaming services providing a stream of accessible entertainment beyond any caliber of what we have seen before. I’m not an expert in art history, but there are clear markers in time we use to identify art from different eras, and these markers are identified through a mix of stylistic and cultural shifts that influence the art of the time."
  },
  {
    "objectID": "posts/23-01-17-art-ai/index.html#concluding-remarks",
    "href": "posts/23-01-17-art-ai/index.html#concluding-remarks",
    "title": "Art and AI: A new era for art?",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\nI thought about this topic for a while before writing it up, and this is now my current response to any questions surrounding art and AI. Sure, it leaves open a lot of questions, like “what is art?”, but I think this is a great opportunity for me to slow down and reevaluate the way I interact with art. When I mindlessly scroll through instagram, am I fully appreciating the humanity and stories of the people who created the work? If I play music while I’m studying or at the gym, is that limiting my focus that should be directed at the musicians? My answer to these questions is no, as is probably the case for many who are reading this. The quality of a conversation isn’t only dictated by the dialogue of the speaker, it requires the presence and attention of the listener. To that end, I want to challenge myself moving forward to wrestle more with this question by taking more time to fully immerse myself in various arts. If AI were to take over art, it’ll be when we stop connecting through the experiences."
  },
  {
    "objectID": "posts/21-07-15-diamonds/index.html",
    "href": "posts/21-07-15-diamonds/index.html",
    "title": "Diamonds",
    "section": "",
    "text": "There is an increasing demand and interest in being able to prove that diamonds are ethically and sustainably sourced. There are several organizations working on creating effective tracking methods using cutting edge technology that combine various tools including blockchain, machine learning, and much more. I heard about this problem from my advisor Michael Goodrich who was wondering if we would be able to come up with effective algorithms to increase the accuracy and effectiveness of this process throughout various stages in the process.\nThere are many interesting questions to be asked about the efficacy and impact having these systems could have, which I might discuss in another post. This post will focus on the question from a purely computer science perspective.\nA diamond goes through several stages before it is sold. After collecting the raw diamonds, these diamonds are sliced into half, then finally carved and polished to look like the jewel you and I are accustomed to seeing. The main role in the tracking systems is to be able to follow a diamond going through this process, and making sure that it is being processed at each stage by organizations committed to ethical and sustainably sourced diamond production.\nTo this end, at each stage of the process, the tracking systems take a snap shot of the diamond in the form of a 3D scan, and uploads it to the database. Having a record of these diamonds should be able to allow users to verify that the diamond didn’t switch midway through the pipeline, which could imply unverified vendors adding their products midway through the system.\nGiven the way current 3D scanning technology works, the results of these scanned objects is not a perfect reconstruction of the object within the computer, but usually a set of points sampled from various parts of the surface object that can approximate the scanned object. This set of points is called a point cloud. Usually, our eyes can process these points and effectively perceive what shape these points form, but it’s not easy to get a computer to understand the point cloud in the way that we do.\nSo now we know that the objects we will be handling in this problem are massive amounts of point clouds of diamonds at various stages. Given these objects, some of the key questions we are interested in exploring are:\n\nGiven two different scans of the same diamond in the same stage, is there an effective way to determine whether they are the same or not?\nGiven two different scans of the same diamond in different stages of the manufacturing process, can we determine which part of the earlier-stage diamond the later-stage diamond came from?\nDiamonds generally have very rigid shapes, even in raw form. Given a scan of points, can we determine what polytope (fancy word for shapes like cubes, tetrahedrons, and other three-dimensional objects with flat sides) it is?\n\nI’ll use this platform to summarize some concepts I learn along the way that might be useful in tackling some of these problems, and hopefully have some updates in the future on any successes!"
  },
  {
    "objectID": "posts/22-08-14-diamonds/index.html",
    "href": "posts/22-08-14-diamonds/index.html",
    "title": "Diamonds are Forever in the Blockchain",
    "section": "",
    "text": "About a year ago, I wrote a blog post about some computational solutions that have been proposed to create a more ethical diamond supply chain. Over the last year, I collaborated with some brilliant people to analyze and propose these solutions. We mainly focused on the process of comparing two different scans of diamonds.\nThroughout the diamond manufacturing process, the diamond is mined, polished, and carved often in different locations before it is finally sold to customers. When the diamond transitions between steps, it is often transferred to a new location. One step of interest is to verify that the correct diamonds have been transferred between each location, to make sure the diamond is properly tracked along the supply chain. From our market research, it appears that organizations like Tracr use machine learning techniques to automate this matching process.\nIn our recent work which can be viewed here, we analyze this problem from the perspective of computational geometry and propose an approximation algorithm to match two scans of diamonds to the desired precision. I’ll give a quick overview of the methods we used, and some of the challenges that we leave open for future exploration.\n\nThe Problem\nThe problem setting we consider is the following: After the necessary processing is completed at location A, we are given a polyhedron C that represents the processed diamond. We are also given a set of points S generated by scanning the same diamond once it arrives at location B. Can you verify that point set S has shape C?\nBelow is a sketch of an example instance of the problem in two dimensions.\nThere’s a body of work that offer solutions to a slight variation of this problem known as point-set registration. In this instance, instead of a polyhedron at location A, we are given another set of points, and we want to compute a translation that aligns these two point sets as best as possible. There are many brilliant algorithms that you can read more about on the linked Wikipedia page and perform very well in practice, but none of these offer theoretical guarantees about performance.\nThere’s also a lot of theoretical work about a specific instance of this problem, where instead of a polyhedron you’re given a circle or a sphere, and you want to find the best fitting one for your set of points. However, there has surprisingly not been much work when considering a polyhedron and a set of points. It turns out it is very challenging to handle the degrees of freedom present in the set of points as well as the possible polyhedra that could be given to you. In this work, we simplified the problem to just look at convex polyhedra.\n\n\nOur Tools\nI won’t dive into the details of the algorithm here, but introduce a few tools we used that I found to be very interesting and fun to work with. Polygons and polyhedrons can be defined in many different ways mathematically, but the way we decided to represent them is using what’s called the polyhedral distance function. (The problem is of primary interest in three dimensions, but we also provide a two-dimensional solution and so most figures will use two dimensions. These ideas aren’t hard to generalize to higher dimensions but are much clearer in two dimensions when drawing them. As such, when I use ‘polyhedron’ in the below explanation, this is interchangeable with ‘polygon’ for the two-dimensional case.)\n\n\nPolyhedral Distance Function\n\nIn simple terms, this distance function has a reference “unit ball” of the polyhedron of interest with a center. It then measures the distance between points p and q by first placing the unit ball centered at p, then returning how much you have to scale the unit ball to touch the point q. This distance function doesn’t necessarily have all the “nice” properties mathematicians want in their distance functions, but it works great in a computational setting.\nWith our mathematical definition of the polyhedron in hand, our algorithm computes the C-shaped minimum width annulus (MWA) of the point set S. Our reasoning for this is that if the two scans indeed represent the same diamond, the minimum width annulus should have a width very close to zero, thus providing a good metric for determining the similarity between the two objects.\n\n\nMinimum Width Annulus\n\nThe MWA is defined as the concentric placement of two copies of the polyhedron such that all the points in the scan lie in between the two scans, such that the difference in polyhedral distance from the center to the two placements is minimized.\n\n\nFuture Directions\nIn our paper, we introduce approximation algorithms to solve this problem in a few different settings (eg. is rotation required?) in two and three dimensions. We do this first by noticing that the search region in the middle can be narrowed down systematically to yield an efficient algorithm, even for high levels of precision. The rotation case turns out to be slightly more demanding, and we imagine there are ways to improve our method in this domain.\nAn interesting direction that can also be explored is the notion of Voronoi diagrams for convex polyhedral distance functions. It turns out the standard Voronoi diagram represents the exact solution to this problem in the case that the shape given is a circle or sphere. We imagine a similar property holds for polygons and polyhedra, but there is a lot we don’t quite understand about Voronoi diagrams for these distance functions."
  },
  {
    "objectID": "posts/22-11-26-lglar-ch2/index.html",
    "href": "posts/22-11-26-lglar-ch2/index.html",
    "title": "Hall LGLAR Chapter 2: The Matrix Exponential",
    "section": "",
    "text": "This is a collection of my notes from chapter 2 of Brian Hall’s “Lie Groups, Lie Algebras, and Representations”. From what I know, the matrix exponential is an important operation used to connect Lie groups and Lie algebras. This chapter introduces the matrix exponential and explores properties about it that should lead to insights on the nature of this connection."
  },
  {
    "objectID": "posts/22-11-26-lglar-ch2/index.html#the-exponential-of-a-matrix",
    "href": "posts/22-11-26-lglar-ch2/index.html#the-exponential-of-a-matrix",
    "title": "Hall LGLAR Chapter 2: The Matrix Exponential",
    "section": "The Exponential of a Matrix",
    "text": "The Exponential of a Matrix\nBefore introducing the exponential of a matrix, we begin by defining the Hilbert-Schmidt norm of a matrix. There are multiple norms that can be defined on the space of matrices, but this seems to be the choice for the study of Lie algebras.\nDefinition 1: For any matrix \\(X \\in M_n(\\mathbb{C})\\), we define\n\\[||X|| = \\left( \\sum_{j,k=1}^n |X_{j,k}|^2\\right)^{1/2}.\\]\nThe quantity \\(||X||\\) is called the Hilbert-Schmidt norm of \\(X\\).\nThis definition is basically the definition of a norm of a vector, where we take the standard vector norm after flattening the matrix \\(X\\), making it quite easy to memorize. A really neat property of the Hilbert-Schmidt norm is that it is independent of the basis, and can be computed by the following:\n\\[||X|| = \\left(\\text{tr} (X^*X)\\right)^{1/2}.\\]\n\nNote: This alternative characterization for the Hilbert-Schmidt norm was somewhat confusing for me at first, but I found a nice way for me to visualize the quantity that the norm is capturing. If you recall, the standard inner product \\(\\left< \\cdot, \\cdot \\right>\\) between vectors \\(x, y \\in \\mathbb{C}^n\\) has the following property when combined with an \\(n \\times n\\) matrix \\(A\\):\n\\[\\left<x, Ay\\right> = \\left< A^* x, y \\right>.\\]\nNow if we ask how much the inner product between two unit vectors \\(\\left< x, y\\right>\\) changes after transforming them by a matrix \\(A\\), we get the following expression:\n\\[\\left<Ax, Ay\\right> = \\left<A^*Ax, y\\right>.\\]\nIn a previous blog post, I explored the trace in an effort to find a geometric interpretation for it. One definition I found was that the trace of an operator characterizes how much an operator scales the volume of a unit ball. Under this picture, the average difference between \\(\\left< x, y\\right>\\) and \\(\\left<Ax, Ay\\right>\\), with the two unit vectors \\(x, y\\) sampled uniformly, is upper-bounded by an expression involving the trace of \\(A^*A\\).\n\\[\\int_x \\int_y \\left<x,y\\right> - \\left<Ax, Ay\\right> dx dy = \\int \\int \\left<x,y\\right> - \\left<A^*Ax, y\\right> dx dy = \\int \\int \\left<x,y\\right> - \\text{tr}(A^*A)/n dx dy \\leq 1 - \\text{tr}(A^*A)/n \\]\n\nThe Hilbert-Schmidt norm satisfies the triangle inequality and a consequence of the Cauchy-Schwartz inequality, expressed as the following:\n\n\\(||X + Y|| \\leq ||X|| + ||Y||\\)\n\\(||XY|| \\leq ||X||\\; ||Y||\\)\n\nWe also take a quick trip back to calculus class to refresh on the definition of a matrix exponential.\nDefinition 2: If \\(X\\) is an \\(n \\times n\\) matrix, the exponential of \\(X\\), written \\(e^X\\), is defined by the usual power series\n\\[e^X = \\sum_{m=0}^\\infty \\frac{X^m}{m!}.\\]\nThe text proves using the Hilbert-Schmidt norm that the above series converges for all \\(X \\in M_n(\\mathbb{C})\\), and proves that \\(e^X\\) is a continuous function of \\(X\\).\nHere we list several properties of the matrix exponential.\nProposition 2.3 (Properties of Matrix Exponential): 1. \\(e^0 = I\\). 2. \\((e^X)^* = e^{X^*}\\). 3. \\(e^X\\) is invertible and \\(\\left(e^X\\right)^{-1} = e^{-X}\\). 4. \\(e^{(\\alpha+\\beta)X} = e^{\\alpha X}e^{\\beta X}\\) for all \\(\\alpha, \\beta \\in \\mathbb{C}\\). 5. If \\(XY = YX\\), then \\(e^{X+Y} = e^Xe^Y = e^Ye^X\\). 6. If \\(C\\) is in \\(\\text{GL}(N;\\mathbb{C})\\), then \\(e^{CXC^{-1}} = Ce^XC^{-1}\\).\nProposition 2.4: Let \\(X\\) be a \\(n \\times n\\) complex matrix. Then \\(e^{tX}\\) is a smooth curve in \\(M_n(\\mathbb{C})\\) and\n\\[\\frac{d}{dt} e^{tX} = X e^{tX} = e^{tX} X.\\]\nIn particular,\n\\[\\frac{d}{dt} e^{tX} \\Big|_{t=0} = X.\\]"
  },
  {
    "objectID": "posts/22-11-26-lglar-ch2/index.html#computing-the-exponential",
    "href": "posts/22-11-26-lglar-ch2/index.html#computing-the-exponential",
    "title": "Hall LGLAR Chapter 2: The Matrix Exponential",
    "section": "Computing the Exponential",
    "text": "Computing the Exponential\nWe can now examine specific examples on how we might compute the exponential of a general matrix. Suppose that we have \\(X \\in M_n(\\mathbb{C})\\) with \\(n\\) linearly independent eigenvectors \\(v_1, \\ldots, v_n\\) with eigenvalues \\(\\lambda_1, \\ldots, \\lambda_n\\). Then the matrix can be diagonalized as\n\\[X = CDC^{-1}\\]\nwhere \\(C\\) is the \\(n \\times n\\) matrix with the eigenvectors of \\(X\\) as its columns, and \\(D\\) is the diagonal \\(n \\times n\\) matrix with the eigenvalues on the diagonal. It is easy to show that the matrix \\(e^D\\) is the matrix with diagonal elements \\(e^{\\lambda_1}, \\ldots, e^{\\lambda_n}\\), so by using property 6 from above, we have that\n\\[e^X = C\\begin{pmatrix} e^{\\lambda_1} & & 0 \\\\ & \\ddots & \\\\ 0 & & e^{\\lambda_n} \\end{pmatrix} C^{-1}.\\]\nSome readers may have encountered the matrix exponential when studying differential equations. Consider the following first-order differential equation\n\\[\\frac{d\\mathbf{v}}{dt} = X\\mathbf{v},\\] \\[\\mathbf{v}(0) = \\mathbf{v}_0,\\]\nwhere \\(\\mathbf{v}(t) \\in \\mathbb{R}^n\\) and \\(X\\) is a fixed \\(n \\times n\\) matrix. The solution of this equation is given by\n\\[\\mathbf{v}(t) = e^{tX}\\mathbf{v}_0,\\]\nwhich can be verified via proposition 2.4."
  },
  {
    "objectID": "posts/22-11-26-lglar-ch2/index.html#the-matrix-logarithm",
    "href": "posts/22-11-26-lglar-ch2/index.html#the-matrix-logarithm",
    "title": "Hall LGLAR Chapter 2: The Matrix Exponential",
    "section": "The Matrix Logarithm",
    "text": "The Matrix Logarithm\nWe also want to define an inverse function to the matrix exponential, which will be called the matrix logarithm. The text describes how this can be done by using techniques and knowledge about the logarithm of complex numbers. I’ll just state the resulting definition here.\nDefinition 3: For an \\(n \\times n\\) matrix \\(A\\), define \\(\\log A\\) by\n\\[\\log A = \\sum_{m=1}^\\infty (-1)^{m+1} \\frac{(A-I)^m}{m}\\]\nwhenever the series converges.\nLike the situation with complex numbers, this function cannot be defined for general matrices, as is formalized in the following theorem.\nTheorem 2.8. The function \\(\\log A\\) is defined and continuous on the set of all \\(n \\times n\\) complex matrices \\(A\\) with \\(||A - I|| < 1\\). For all \\(A \\in M_n(\\mathbb{C})\\) with \\(||A - I|| < 1,\\)\n\\[e^{\\log A} = A.\\]\nFor all \\(X \\in M_n(\\mathbb{C})\\) with \\(||X|| < \\log 2\\), \\(||e^X - I|| < 1\\) and\n\\[\\log e^X = X.\\]\nFinally, the following proposition gives us a way to upperbound the matrix logarithm by a function of the matrix’ Hilbert-Schmidt norm.\nProposition 2.9. There exists a constant \\(c\\) such that for all \\(n \\times n\\) matrices \\(B\\) with \\(||B|| < 1/2\\), we have\n\\[||\\log(I + B) - B|| \\leq c ||B||^2.\\]\nThis can be restated in a more concise way by\n\\[\\log(I + B) = B + O(||B||^2).\\]"
  },
  {
    "objectID": "posts/22-11-26-lglar-ch2/index.html#further-properties-of-the-exponential",
    "href": "posts/22-11-26-lglar-ch2/index.html#further-properties-of-the-exponential",
    "title": "Hall LGLAR Chapter 2: The Matrix Exponential",
    "section": "Further Properties of the Exponential",
    "text": "Further Properties of the Exponential\nThis section introduces several key properties of the matrix exponential that will lead into the discussion of Lie algebras. The first is a familiar equation for those who have seen the Trotter product formula for quantum simulation algorithms.\nTheorem 2.11 (Lie Product Formula). For all \\(X, Y \\in M_n(\\mathbb{C})\\), we have \\[e^{X + Y} = \\underset{m \\rightarrow \\infty}{\\lim} \\left( e^{X/m} e^{Y/m}\\right)^m.\\]\nTheorem 2.12. For any \\(X \\in M_n(\\mathbb{C})\\), we have \\[\\det\\left(e^X\\right) = e^{\\text{trace}(X)}.\\]\nProposition 2.16. The exponential map is an infinitely differentiable map of \\(M_n(\\mathbb{C})\\) into \\(M_n(\\mathbb{C})\\)."
  },
  {
    "objectID": "posts/22-11-26-lglar-ch2/index.html#polar-decomposition",
    "href": "posts/22-11-26-lglar-ch2/index.html#polar-decomposition",
    "title": "Hall LGLAR Chapter 2: The Matrix Exponential",
    "section": "Polar Decomposition",
    "text": "Polar Decomposition\nFinally, I summarize some quick notes on the polar decomposition of a matrix. The introduction to the topic as a generalization of polar decompositions for complex numbers was very interesting, so I suggest taking a look for people who want a better understanding of the topic.\nTheorem 2.17. 1. Every \\(A \\in GL(n;\\mathbb{C})\\) can be written uniquely in the form \\[A = UP\\] where \\(U\\) is unitary and \\(P\\) is self-adjoint and positive. 2. Every self-adjoint positive matrix \\(P\\) can be written uniquely in the form \\[P = e^X\\] with \\(X\\) self-adjoint. Conversely, if \\(X\\) is self-adjoint, then \\(e^X\\) is self-adjoint and positive. 3. If we decompose each \\(A \\in GL(n;\\mathbb{C})\\) uniquely as \\[A = Ue^X\\] with \\(U\\) unitary and \\(X\\) self-adjoint, then \\(U\\) and \\(X\\) depend continuously on \\(A\\)."
  },
  {
    "objectID": "posts/22-11-24-lglar-ch1/index.html",
    "href": "posts/22-11-24-lglar-ch1/index.html",
    "title": "Hall LGLAR Chapter 1: Matrix Lie Groups",
    "section": "",
    "text": "Collection of some notes and solutions to exercises from Brian Hall’s “Lie Groups, Lie Algebras, and Representations”. My primary motivation for going through this literature around Lie groups is to understand the unitary groups and special unitary groups better, so the problems I solve will be focused on those groups more than others. The definition numberings are assigned by myself to reference in the exercise solutions."
  },
  {
    "objectID": "posts/22-11-24-lglar-ch1/index.html#definitions",
    "href": "posts/22-11-24-lglar-ch1/index.html#definitions",
    "title": "Hall LGLAR Chapter 1: Matrix Lie Groups",
    "section": "Definitions",
    "text": "Definitions\nDefinition 1: The general linear group over \\(\\mathbb{R}\\), \\(\\text{GL}(n; \\mathbb{R})\\), is the group of all \\(n\\) by \\(n\\) invertible matrices with real entries, where the group operation is the standard matrix product. A similar definition is used to define \\(\\text{GL}(n; \\mathbb{C})\\).\nDefinition 2: \\(M_n(\\mathbb{C})\\) is the space of all \\(n\\) by \\(n\\) matrices with complex entries.\nDefinition 3: Let \\(A_m\\) be a sequence of complex matrices in \\(M_n(\\mathbb{C})\\). We say that \\(A_m\\) converges to a matrix \\(A\\) if each entry of \\(A_m\\) converges to its corresponding entry in \\(A\\).\nDefinition 4: A matrix Lie group is a subgroup \\(G\\) of \\(\\text{GL}(n;\\mathbb{C})\\) with one of the following properties: 1. If \\(A_m\\) is any sequence of matrices in \\(G\\), and \\(A_m\\) converges to some matrix \\(A\\), then either 1. \\(A \\in G\\) 2. \\(A\\) is not invertible. 2. \\(G\\) is a closed subgroup of \\(\\text{GL}(n;\\mathbb{C})\\).\nDefinition 5: A matrix \\(A\\) is unitary if 2 column vectors are orthonormal, i.e. \\(\\sum_{l=1}^n \\overline{A}_{lj}A_{lk}=\\delta_{jk}\\). Equivalently, a unitary matrix is one that satisfies \\(A^* A = AA^* = I\\)."
  },
  {
    "objectID": "posts/22-11-24-lglar-ch1/index.html#examples-of-matrix-lie-groups",
    "href": "posts/22-11-24-lglar-ch1/index.html#examples-of-matrix-lie-groups",
    "title": "Hall LGLAR Chapter 1: Matrix Lie Groups",
    "section": "Examples of Matrix Lie Groups",
    "text": "Examples of Matrix Lie Groups\n1. Unitary group: The collection of unitary matrices forms a subgroup of \\(\\text{GL}(n;\\mathbb{C})\\) called the unitary group, denoted by \\(U(n)\\). We can also define the special unitary group, \\(SU(n)\\), as the subgroup of \\(U(n)\\) with all matrices having determinant 1.\n\nQuick check 1: Show that \\(G = U(n)\\) (or \\(G = SU(n)\\)) is a matrix Lie group.\nPROOF: To show this, it suffices to show that \\(G\\) satisfies one of the properties in definition 4. From the text, it seems like it is a trivial exercise to show condition 2 holds, so I prove that here (although I’m not sure if this is the cleanest way to prove it).\nThe goal is to show that \\(G\\) is a closed subgroup of \\(\\text{GL}(n;\\mathbb{C})\\). This means that \\(G\\) must be closed as a group, as well as a topological subspace. First to show it is a closed group, we must show \\(U_1U_2 \\in G\\) and \\(U^{-1} \\in G\\) for all \\(U, U_1, U_2 \\in G\\).\n\nSince \\((U_1U_2)^*(U_1U_2) = U_2^* U_1^* U_1 U_2 = I\\), \\(U_1U_2 \\in G\\).\nFor \\(U \\in G\\), \\((UU^{-1})^* = I^* = I\\). By standard matrix identities, \\((UU^{-1})^* = (U^{-1})^*U^* = I\\), implying that \\((U^{-1})^* = (U^*)^{-1}\\). Using this, we have that \\((U^{-1})^* U^{-1} = (U^*)^{-1} U^{-1} = (UU^*)^{-1} = I\\), proving that \\(U^{-1} \\in G\\).\n\nTo show that \\(G\\) is a topologically closed set, first define the function \\(f: U(n) \\rightarrow U(n)\\) as \\(f(A) = A^* A\\). This can be shown to be a continuous function. Furthermore, we can define \\(U(n)\\) as the preimage of \\(\\{I_n\\}\\), a closed set, under this continuous function, showing that \\(U(n)\\) is indeed a closed topological set. To see that this holds for \\(SU(n)\\), a similar proof can be used to show that \\(SL(n\\; \\mathbb{C})\\) is a closed set. Since the intersection of two closed sets is closed, we get \\(SL(n; \\mathbb{C}) \\cap U(n) = SU(n)\\) is closed too.\n\n2. Symplectic group: Consider the skew-symmetric bilinear form \\(B\\) on \\(\\mathbb{R}^{2n}\\) defined by the following:\n\\[\\omega(x, y) = \\sum_{j=1}^n (x_jy_{n+j} - x_{n+j}y_j).\\]\nThe set of all \\(2n \\times 2n\\) matrices \\(A\\) which preserve \\(\\omega\\) is the real symplectic group \\(\\text{Sp}(n;\\mathbb{R})\\), and it is a closed subgroup of \\(\\text{GL}(2n;\\mathbb{R})\\). If \\(\\Omega\\) is the \\(2n \\times 2n\\) matrix\n\\[\\Omega = \\begin{pmatrix}0 & I \\\\ I & 0\\end{pmatrix},\\]\nthen\n\\[\\omega(x,y) = \\left< x, \\Omega y \\right>.\\]\nFrom this, we can show that a \\(2n \\times 2n\\) real matrix \\(A\\) belongs to \\(\\text{Sp}(n;\\mathbb{R})\\) if and only if\n\\[-\\Omega A^{T} \\Omega = A^{-1}.\\]"
  },
  {
    "objectID": "posts/22-11-24-lglar-ch1/index.html#topological-properties",
    "href": "posts/22-11-24-lglar-ch1/index.html#topological-properties",
    "title": "Hall LGLAR Chapter 1: Matrix Lie Groups",
    "section": "Topological Properties",
    "text": "Topological Properties\nThere are three main topological properties we are interested in that a matrix Lie group can satisfy, which are stated in this section.\n\n1. Compactness\nDefinition 6: A matrix Lie group \\(G \\subset \\text{GL}(n;\\mathbb{C})\\) is said to be compact if it is compact in the usual sense as a subset of \\(M_n(\\mathbb{C}) \\cong \\mathbb{R}^{2n^2}\\).\nThe ``usual sense’’ of compactness is hard to get a sense of for those with limited exposure to topology, but thankfully the following theorem provides necessary and sufficient conditions for this property to hold, at least for subsets of Euclidean spaces.\nHeine-Borel Theorem: For a subset \\(S\\) of Euclidean space \\(\\mathbb{R}^n\\), the following two statements are equivalent:\n\n\\(S\\) is closed and bounded\n\\(S\\) is compact, that is, every open cover of S has a finite subcover.\n\nHere, closed means that for any sequence \\(A_m \\in S\\) such that \\(A_m \\rightarrow A\\), then \\(A \\in S\\). We say a subset \\(G \\subset M_n(\\mathbb{C})\\) is bounded if there exists a constant \\(C\\) such that for all \\(A \\in G\\), we have \\(|A_{jk}| \\leq C\\) for all \\(1 \\leq j,k \\leq n\\).\n\nQuick check 2: Show that \\(U(n)\\) and \\(SU(n)\\) are compact.\nPROOF: From quick check 1, we know that \\(U(n)\\) and \\(SU(n)\\) are closed. They are also bounded, since by definition, the columns of matrices in these groups must be unit vectors. This means that \\(|A_{jk}| \\leq 1\\) for all \\(1 \\leq j, k \\leq n\\). By the Heine-Borel theorem, we concluded that these two groups are indeed compact.\n\n\n\n2. Connectedness\nDefinition 7: A matrix Lie group \\(G\\) is path connected if for all \\(A\\) and \\(B\\) in \\(G\\), there exists a continuous path \\(A(t)\\), \\(a \\leq t \\leq b\\), lying in \\(G\\) with \\(A(a) = A\\) and \\(A(b) = B\\). For any matrix Lie group \\(G\\), the identity component of \\(G\\), denoted by \\(G_0\\), is the set of \\(A \\in G\\) for which there exists a continuous path \\(A(t)\\), \\(a \\leq t \\leq b\\), lying in \\(G\\) with \\(A(a) = I\\) and \\(A(b) = A\\).\nIt was stated without proof that for the case of matrix Lie groups, path connectedness implies connectedness, so the two properties are used interchangeably.\n\nQuick check 3: Show that \\(U(n)\\) and \\(SU(n)\\) are connected for all \\(n \\geq 1\\).\nPROOF: Every unitary matrix \\(U\\) has the property that it can be diagonalized, i.e., written as \\(U = VDV^{-1}\\) where \\(V \\in U(n)\\) and \\(D\\) is diagonal with diagonal entries \\(e^{i\\theta_1}, \\ldots, e^{i\\theta_n}\\). We can then define\n\\[U(t) = V\\begin{pmatrix} e^{i(1-t)\\theta_1} & & 0 \\\\  & \\ddots &  \\\\ 0 &  & e^{i(1-t)\\theta_n} \\end{pmatrix}V^{-1}, 0\\leq t \\leq 1.\\]\nIt is clear that \\(U(t)\\) stays in \\(U(n)\\) for all \\(t\\), and \\(U(t)\\) connects \\(U\\) to \\(I\\), showing that \\(U(n)\\) is indeed connected.\nWe can do a similar construction for \\(SU(n)\\), but with the modification that the \\(n\\)-th diagonal element is the inverse of the product of the first \\(n - 1\\) elements, ensuring that the determinant is 1.\n\n\n\n3. Simple Connectedness\nDefinition 8: A matrix Lie group is simply connected if it is connected and, in addition, every loop in \\(G\\) can be shrunken continuously to a point in \\(G\\). More precisely, assume \\(G\\) is connected. Then, \\(G\\) is simply connected if for every continuous path \\(A(t)\\), \\(0 \\leq t \\leq 1\\), lying in \\(G\\) and with \\(A(0) = A(1)\\), there exists a continuous function \\(A(s, t)\\), \\(0 \\leq s, t \\leq 1\\), taking values in \\(G\\) and having the following properties: 1. \\(A(s, 0) = A(s, 1)\\) for all \\(s\\) 2. \\(A(0, t) = A(t)\\) 3. \\(A(1, t) = A(1, 0)\\) for all \\(t\\).\nIt turns out that \\(SU(n)\\) is simply connected for all \\(n\\), but the author defers the proof to a later chapter, possibly implying that we don’t have the necessary tools yet to demonstrate this. However, it is not hard to show that \\(SU(2)\\) is simply connected, as this space (which is the space of states a single qubit can take), is isomorphic to the unit sphere, and the above properties can easily be demonstrated."
  },
  {
    "objectID": "posts/22-11-24-lglar-ch1/index.html#exercises",
    "href": "posts/22-11-24-lglar-ch1/index.html#exercises",
    "title": "Hall LGLAR Chapter 1: Matrix Lie Groups",
    "section": "Exercises",
    "text": "Exercises\nA collection of my solutions to select exercises. The plan is to choose the exercises related to \\(U(n)\\) and \\(SU(n)\\).\n\nExercise 3:\nShow that \\(\\text{Sp}(1;\\mathbb{C}) = \\text{SL}(2;\\mathbb{C})\\) and that \\(\\text{Sp}(1) = SU(2)\\).\nDefinition: The compact symplectic group \\(\\text{Sp}(2)\\) is defined as \\(\\text{Sp}(2) \\equiv \\text{Sp}(1;\\mathbb{C}) \\cap U(2)\\).\nPROOF: Let \\(A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} \\in \\text{SL}(2;\\mathbb{C})\\). If suffices to show that \\(-\\Omega A^T \\Omega = A^{-1}\\) for \\(\\Omega\\) defined above. Since \\(\\det(A) = 1\\), we know that \\(A^{-1} = \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}\\).\n\\[-\\Omega A^T \\Omega = \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} a & c \\\\ b & d \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix} = \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}.\\]\nSo \\(\\text{Sp}(1;\\mathbb{C}) = \\text{SL}(2;\\mathbb{C})\\).\n(\\(\\text{Sp}(2) \\subset SU(2)\\)). Let \\(A \\in SU(2)\\). Then we have \\(\\det(A) = 1\\) and \\(A^*A = I\\) so \\(A \\in SU(2)\\).\n(\\(\\text{Sp}(2) \\supset SU(2)\\)). Let \\(A \\in SU(2)\\). Since \\(SU(2) \\subset U(2)\\) holds by definition, it suffices to show that \\(SU(2) \\subset \\text{Sp}(1; \\mathbb{C})\\). This proof is equivalent to the first part of the problem.\n\\(\\Rightarrow\\) \\(\\text{Sp}(1) = SU(2)\\).\n\nExercise 5 (Part 1): Show that if \\(\\alpha\\) and \\(\\beta\\) are arbitrary complex numbers satisfying \\(|\\alpha|^2 + |\\beta|^2 = 1\\), then\n\\[A = \\begin{pmatrix}\\alpha & -\\overline{\\beta} \\\\ \\beta & \\overline{\\alpha}\\end{pmatrix} \\in SU(2).\\]\nPROOF: Let \\(\\alpha\\) and \\(\\beta\\) be complex numbers satisfying \\(|\\alpha|^2 + |\\beta|^2 = 1\\). Use these to construct the matrix \\(A = \\begin{pmatrix}\\alpha & -\\overline{\\beta} \\\\ \\beta & \\overline{\\alpha}\\end{pmatrix}\\). Then, * \\(\\det{A} = |\\alpha|^2 + |\\beta|^2 = 1\\), and * \\(A^* A = \\begin{pmatrix} \\overline{\\alpha} & \\overline{\\beta} \\\\ -\\beta & \\alpha \\end{pmatrix} \\begin{pmatrix}\\alpha & -\\overline{\\beta} \\\\ \\beta & \\overline{\\alpha}\\end{pmatrix} = I\\).\nSo \\(A \\in SU(2)\\).\nExercise 5 (Part 2): Show that every \\(A \\in SU(2)\\) can be expressed in the above form for a unique pair \\((\\alpha, \\beta)\\) satisfying \\(|\\alpha|^2 + |\\beta|^2\\).\nBegin by taking \\(A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\).\nSince \\(\\det A = 1\\) by definition of \\(SU(2)\\). Also by definition of \\(SU(2)\\), we have that \\(A^{-1} = A^*\\). By definition of inverses of \\(2 \\times 2\\) matrices, we have\n\\[A^{-1} = \\frac{1}{\\det{A}} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix},\\]\nand\n\\[A^* = \\begin{pmatrix} \\overline{a} & \\overline{b} \\\\ \\overline{c} & \\overline{d} \\end{pmatrix}.\\]\nThis implies that \\(d = \\overline{a}\\) and \\(b = -\\overline{c}\\).\nFinally, we know that \\(A^* A = I\\), implying that \\(|a|^2 + |c|^2 = 1\\).\nWe conclude that \\(A = \\begin{pmatrix} a & -\\overline{c} \\\\ c & \\overline{a} \\end{pmatrix}\\) for \\(|a|^2 + |c|^2 = 1\\).\n\nExercise 9: Suppose \\(a\\) is an irrational real number. Show that the set \\(E_a\\) of numbers of the form \\(e^{2\\pi i n a}\\) for \\(n \\in \\mathbb{Z}\\) is dense in the unit circle \\(S^1\\).\nPROOF: First we show that for any \\(n_1 \\neq n_2 \\in \\mathbb{Z}\\), \\(e^{2\\pi i n_1 a} \\neq e^{2\\pi i n_2 a}\\). Without loss of generality, assume that \\(n_1 > n_2\\). Suppose that there was a pair of integers such that \\(e^{2\\pi i n_1 a} = e^{2\\pi i n_2 a}\\). Then,\n\\[e^{2\\pi i n_1 a} - e^{2\\pi i n_2 a} = 0\\] \\[\\Rightarrow \\cos(2\\pi n_1 a) + i \\sin(2\\pi n_1 a) - \\cos(2\\pi n_2 a) - i \\sin (2\\pi n_2 a) = 0\\] \\[\\Rightarrow -2\\sin((2\\pi(n_1 + n_2)a)/2) \\sin((2\\pi(n_1 - n_2)a)/n) + i2\\cos((2\\pi(n_1+n_2)a)/2)\\sin((2\\pi(n_1-n_2)a)/2) = 0\\]\nIt’s simple to show that if any of the values in the parentheses of the sin and cos functions above, it contradicts \\(a\\) being irrational. From this, we see that \\(n_1 \\neq n_2 \\Rightarrow e^{2\\pi i n_1 a} \\neq e^{2\\pi i n_2 a}\\).\nNow consider dividing the unit circle \\(S^1\\) into \\(N\\) evenly sized parts, each with length \\(2\\pi / N\\). Since \\(E_a\\) contains as many elements as there are natural numbers, here must be at least one region that contains an infinite number of points. Since \\(E_a\\) is a subgroup of \\(S^1\\) and the group operation is equivalent to a rotation on the circle, we can rotate the bin with infinite elements to each of the other \\(N - 1\\) arcs, while remaining in the group \\(E_a\\). Now choose \\(N\\) to be the inverse of any \\(\\epsilon < 0\\), and we have shown that \\(E_a\\) is dense in \\(S^1\\)."
  },
  {
    "objectID": "posts/23-11-03-how-to-study-for-more-than-your-midterm/index.html",
    "href": "posts/23-11-03-how-to-study-for-more-than-your-midterm/index.html",
    "title": "How to Study for (More Than) Your Algorithms Midterm",
    "section": "",
    "text": "Many students asked about this, so I thought I would write up my suggestions on how you should go about studying your midterms. This is only one suggestion, so make sure to tailor it better for where you are at right now and what you need. The cool thing about learning is that you can learn about it too! The more you experiment with how to learn, the better you will get at it. Don’t settle for learning about algorithms, use this as an opportunity to learn how to learn better as well.\nYou can skip the Context section to the How to Study section (Section 2) for a tl;dr of actionable steps I suggest, but I encourage you to read and think about the context as well!"
  },
  {
    "objectID": "posts/23-11-03-how-to-study-for-more-than-your-midterm/index.html#my-suggestion",
    "href": "posts/23-11-03-how-to-study-for-more-than-your-midterm/index.html#my-suggestion",
    "title": "How to Study for (More Than) Your Algorithms Midterm",
    "section": "1.1 My Suggestion",
    "text": "1.1 My Suggestion\nAs someone on your teaching staff, we would love if all of you can achieve level 6 proficiency. But of course this is a lofty expectation, and probably doesn’t align with what you are hoping to get out of this course. I can identify two potential realistic and plausible goals that most students have: 1. Get a good (or even passing) grade so you can get your degree 2. Gain familiarity with the concepts so you’re well equipped for coding interviews. I think for these purposes, shooting for level 4 is a great goal to have for most of the concepts covered in the course.\nFor this goal, I think it is critical that you are very good at level 3, and actively practice level 4 throughout the quarter. Fortunately, we live in a time where you have a lot of resources to get to level 3! Not only that, my suggestion is that you start at level 3! The way the hierarchy works, you will naturally be forced to pick up on lower level cognitive abilities as you practice higher levels. The caveat is that if you start too high up, you will be way too lost to even try anything! In that sense, level 3 is the sweet spot."
  },
  {
    "objectID": "posts/23-11-03-how-to-study-for-more-than-your-midterm/index.html#notes-on-level-3---apply",
    "href": "posts/23-11-03-how-to-study-for-more-than-your-midterm/index.html#notes-on-level-3---apply",
    "title": "How to Study for (More Than) Your Algorithms Midterm",
    "section": "1.2 Notes on Level 3 - Apply",
    "text": "1.2 Notes on Level 3 - Apply\nLike I mentioned above, level 3 for our purposes is the ability to correctly apply Big O analysis for new algorithms and problems you see. Lucky for you, this happens to align with what many coding interviews will ask of you as well! They will tell you a problem, have you propose a solution, then analyze your solution. The reason I don’t think this is a level 6 skill, is that most times the solution they have in mind is just some special version of one of the X number of algorithms they will test you on. Because of this, it’s an application of your knowledge of those algorithms in a new setting. You will at least learn the names of most of these algorithms in this class.\nI’ll also give you an explicit example of the difference between level 2 and level 3. To get to level 2, you should be able to read a proof, and verify that it is correct. On the other hand, true level 3 mastery would mean that if suddenly the proof was not available anymore, you would be able to write the proof yourself!\nAnother example of the difference is a coding exercise. Level 2 mastery would involve looking at a solution to a leetcode problem, and verifying that it works correctly. On the other hand, level 3 mastery would be when you are able to write that solution on your own. The difference is subtle, but I hope you agree with me that we want to achieve level 3!\nI think one wall many students face is that they don’t recognize that these two actions are at different cognitive levels, and feel discouraged when they can’t come up with a solution that looks so simple in hindsight.\n(SIDENOTE: If you find this difference between verifying and solving to be interesting… theoretical computer science may be exactly the field you want to explore!)"
  },
  {
    "objectID": "posts/23-11-03-how-to-study-for-more-than-your-midterm/index.html#the-learning-cycle",
    "href": "posts/23-11-03-how-to-study-for-more-than-your-midterm/index.html#the-learning-cycle",
    "title": "How to Study for (More Than) Your Algorithms Midterm",
    "section": "1.3 The Learning Cycle",
    "text": "1.3 The Learning Cycle\nThere is a lot of research people have done to explore what it takes to achieve level 3 type mastery. To place it in context, this is things like 1) going from appreciating art to learning how to draw, 2) going from watching professional sports players and identifying good and bad plays to learning how to play the sport yourself, or 3) going from enjoying good food to learning how to cook. What many people are taught is that to get good at these things, you have to practice. Many of us have heard of the 10,000 hour rule, stating that to become proficient at something you have to practice it for 10,000 hours. There is a similar idiom in Japan from bonsai fans saying “Watering plants for three years”, meaning that even for something that seems trivial like watering your plants, it takes 3 years to gain mastery of it.\nWhat research has shown though, is that this is not the full picture! “Practice”, is not sufficient, at least in the way we are used to thinking about it. What is required is high quality practice. This can be summarized by iterating through what I’ve seen called the learning cycle:\nwhile (not proficient)\n    1. Do\n    2. Reflect\n    3. Learn\nTraditional ideas of practice only focused on repeating the “do” part, but what we know now is that it is critical to reflect, and reflect on our process of making decisions as well. The more times you go through this cycle, the better you will be at a subject!"
  },
  {
    "objectID": "posts/23-11-03-how-to-study-for-more-than-your-midterm/index.html#how-to-cycle-effectively",
    "href": "posts/23-11-03-how-to-study-for-more-than-your-midterm/index.html#how-to-cycle-effectively",
    "title": "How to Study for (More Than) Your Algorithms Midterm",
    "section": "1.4 How to Cycle Effectively",
    "text": "1.4 How to Cycle Effectively\nHere is an example of how you would do this for our class.\n\n1.4.1 1. Do\nThe first thing you need to do is to try solving problems outside your comfort zone. Set a timer that forces you to spend only 20 minutes to solve the problem. Try to remember everything you can that seems related to the problem, see if you can combine them effectively to solve it.\nAs soon as the timer goes off, STOP! Our goal is not to mull over one question forever, it is to go through this cycle as many times as we can.\n\n\n1.4.2 2. Reflect\nLook up a solution to the problem. Look through the steps you took and see where you made a mistake. Make sure you verbalize and identify one missing piece you had by saying it out loud or writing it down explicitly. - “I didn’t think of using a sorting algorithm as a subroutine” - “I didn’t take into account the time complexity of printing all the leaves of the tree” - “I am misunderstanding big O notation, because their explanation for the time complexity doesn’t match with what I said” - “I know the algorithm is O(n log n) because mergesort is the most expensive part of it, but I can’t explain why mergesort is O(n log n)” - etc.\nIf you can’t identify your mistake, this is the greatest way to take advantage of being at a university! You could post a question on stack overflow, but there are people around you who are willing to help! Discuss the problem and your solution with a peer, a TA, or professors. Reflection on your own is hard, and you’ll get better as you go, but it can be accelerated significantly by having an expert chime in and guide you along this process. Try to learn how the expert goes about identifying the issue, and what kind of advice they propose as a solution.\n\n\n1.4.3 3. Learn\nNow that you know what your mistake was, this is the time to climb down Bloom’s taxonomy and reinforce your memory (Level 1: definitions, theorem statements, etc.) or refresh on the pseudocode of algorithms (Level 2: understand, describe).\nAt the end of this process, make sure you identified 1 thing to cycle through! In fact, I encourage you to keep it at 1 thing so you really focus your attention on the given topic."
  },
  {
    "objectID": "posts/23-11-03-how-to-study-for-more-than-your-midterm/index.html#why-this-is-a-great-time-to-learn",
    "href": "posts/23-11-03-how-to-study-for-more-than-your-midterm/index.html#why-this-is-a-great-time-to-learn",
    "title": "How to Study for (More Than) Your Algorithms Midterm",
    "section": "1.5 Why this is a great time to learn",
    "text": "1.5 Why this is a great time to learn\nWe intuitively think of some things as easier to learn than others. It’s easier to learn how to ride a bike than it is to bake a cake from scratch. We now have a tool to explain why this might be the case! Activities like learning how to ride a bike have an almost ideal learning cycle. First, the space of actions is small (balance your torso, steer the handle, press the pedals), and second, the iterations are fast (you fall down, but it’s quick to get up and try again). The space of actions being small means that you don’t have a hard time coming up with what your potential mistake was during reflection. Contrast this with something like baking, if your cake tastes bad, it could have been because you messed up the proportions of the ingredients (which has many possible combinations!), the temperature of the oven, etc..\nProving mathematical statements is in a sense harder because the space of possibilities is HUGE! It is really hard to learn how to effectively navigate the space of possible actions. However, if we focus our attention on a single topic (algorithm design and analysis), we can constrain that space to a familiar set of tools, and significantly reduce the search space.\nFurthermore, you have many different exercises to try from your textbooks, but also many great external resources about these topics! Leetcode is a great resource to quickly iterate through the learning cycle, because it is 1) focused in scope, and 2) you get quick feedback on correctness! Fortunately for us, there is a lot of overlap between the practice you get on websites like Leetcode and the content you learn in this class."
  },
  {
    "objectID": "posts/23-11-03-how-to-study-for-more-than-your-midterm/index.html#maintain-a-list-of-topics-learned-in-this-class",
    "href": "posts/23-11-03-how-to-study-for-more-than-your-midterm/index.html#maintain-a-list-of-topics-learned-in-this-class",
    "title": "How to Study for (More Than) Your Algorithms Midterm",
    "section": "2.1 Maintain a list of topics learned in this class",
    "text": "2.1 Maintain a list of topics learned in this class\nThe slides are a great place to refresh on the lectures, but you should actively try to create a new document that highlights the topics you learn in the class. This will give you a centralized place to refer to when trying to figure out what you have to study. The midterm 1 study guide is a great way to start this list!"
  },
  {
    "objectID": "posts/23-11-03-how-to-study-for-more-than-your-midterm/index.html#pick-a-topic-each-day-to-practice",
    "href": "posts/23-11-03-how-to-study-for-more-than-your-midterm/index.html#pick-a-topic-each-day-to-practice",
    "title": "How to Study for (More Than) Your Algorithms Midterm",
    "section": "2.2 Pick a topic each day to practice",
    "text": "2.2 Pick a topic each day to practice\nWe want to focus our attention when learning. Pick one or two topics from your list that you feel you need to practice, and find problems related to those. A good source from our class textbook is the “Creativity” section of the exercises. You could also go to leetcode and search from problems related to the selected topic. Leetcode problems have tags, for example the tag greedy gives a list of problems related to greedy algorithms. Try to practice on problems that are level medium or higher.\nIf you are rusty on the math background, there are also many automated problem generators to serve as a source of practice for you too! Khan academy is a great one, but I’m sure there are many other ones too. If you struggled with the math prerequisites, I encourage you to spend a little bit of time practicing them too.\nAnother source of problems that you are forced to solve are the homework problems. Identify which topic we are trying to help you practice, and go through the steps below.\n\n2.2.1 Do!\nSet a timer for 15 minutes and do your best to solve the problem. The first few times you try this, you should not be able to do it, so don’t be discouraged if it is hard. Remember, our goal is to iterate through the learning cycle as many times as we can.\nOnce the timer goes off, 1. If you are close to a solution and feel confident, try for another 10 minutes. At the end of that 10 minutes, move on. 2. If you don’t feel close to solving it, move on to the next step.\n\n\n2.2.2 Reflect\nFind a solution to the problem, and try to compare what you did to what the solution did. Make sure that you understand the solution! This is a good step to spend time on.\n\nIf you got it mostly correct, great job! It’s time to try another problem.\nIf you didn’t get it right, don’t worry! That’s what this whole process is for anyways. For the reflection to be effective, you need to be quite diligent in the do step with recording what you are trying! Identify one key issue between your process and the solution, and explicitly say it out loud of write it down. Verbalize what you are missing so you can really pinpoint what knowledge you were missing.\n\nIf you really can’t identify where you went wrong, feel free to ask us! Table it for your study session, and bring the problem to an office hour. We can try to figure it out together.\n\nFor a homework problem, you may need to wait to do this until the next week. Feel free to ask us about your process though! We are happy to identify what you are doing correctly and what you may need to fix.\n\n\n\n2.2.3 Learn\nNow that you’ve identified what knowledge you were missing (level 1) or concept you were misunderstanding (level 2), spend some time to make sure that doesn’t happen again. Review those missing pieces so you don’t have to repeat this step for the same topic more than 2 times.\n\n\n2.2.4 Wrap Up\nI would say doing this every day for a total of 1.5 hours is a realistic goal. Once you finish, go to your list of topics and put a checkmark next to the topics you practiced that day."
  },
  {
    "objectID": "posts/23-11-03-how-to-study-for-more-than-your-midterm/index.html#identify-connections-in-your-learning",
    "href": "posts/23-11-03-how-to-study-for-more-than-your-midterm/index.html#identify-connections-in-your-learning",
    "title": "How to Study for (More Than) Your Algorithms Midterm",
    "section": "2.3 Identify connections in your learning",
    "text": "2.3 Identify connections in your learning\nOnce you have about 5 checkmarks next to multiple topics, start doing this step too. Go through the list of things you have 5 checkmarks on, and see if those are related in anyway. Do they use similar ideas? Do you like one more than the other? If so, why? Does one of them depend on another idea?\nIdeally at the end of the course, you’ll have something like a mind map of the topics we cover in this course. This step of relating and synthesizing information you learned is a level 4 cognitive task!"
  },
  {
    "objectID": "posts/lglar-ch3/index.html",
    "href": "posts/lglar-ch3/index.html",
    "title": "Hall LGLAR Chapter 3: The Matrix Exponential",
    "section": "",
    "text": "This is a collection of my notes from chapter 3 of Brian Hall’s “Lie Groups, Lie Algebras, and Representations”. In this chapter we are finally introduced to the definition of a Lie algebra, and learn how they are related to Lie groups. In the literature, Lie algebras are often expressed using lowercase Gothic (Fraktur) characters. In latex, these characters can be written using the command “\\mathfrak”."
  },
  {
    "objectID": "posts/lglar-ch3/index.html#definitions-and-first-examples",
    "href": "posts/lglar-ch3/index.html#definitions-and-first-examples",
    "title": "Hall LGLAR Chapter 3: The Matrix Exponential",
    "section": "Definitions and First Examples",
    "text": "Definitions and First Examples\nDefinition 1: A finite-dimensional real or complex Lie algebra is a finite-dimensional real or complex vector space \\(\\frak{g}\\), together with a map \\([\\cdot, \\cdot]\\) from \\(\\frak{g} \\times \\frak{g}\\) into \\(\\frak{g}\\), with the following properties: 1. \\([\\cdot, \\cdot]\\) is bilinear. 2. \\([\\cdot, \\cdot]\\) is skew-symmetric: \\([X, Y] = - [Y, X]\\) for all \\(X, Y \\in \\frak{g}\\). 3. The Jacobi identity holds: \\[[X, [Y, Z]] + [Y, [Z, X]] + [Z, [X, Y]] = 0\\] for all \\(X, Y, Z \\in \\frak{g}\\).\nTwo elements \\(X, Y\\) of a Lie algebra $ $ commute if \\([X, Y] = 0\\). A Lie algebra \\(\\frak{g}\\) is commutative if \\([X, Y] = 0\\) for all \\(X, Y \\in \\frak{g}\\)."
  },
  {
    "objectID": "posts/22-11-15-trace/index.html",
    "href": "posts/22-11-15-trace/index.html",
    "title": "Visualizing the Trace",
    "section": "",
    "text": "When studying quantum computing and quantum information, one frequently occurring concept is that of the trace of an operator. This is quite natural given the setting, as matrices occur as the fundamental building blocks of the field. Given this, we encounter the trace in ideas including distance measures (trace distance) and classes of operators (trace preserving maps). Operationally this quantity isn’t too difficult to compute as we’ll see soon, but I’ve struggled to grasp the intuition behind why it’s so important. In this post I’ll be exploring some neat definitions I’ve found for this quantity and hopefully in the process we can get a better grasp on what this mathematical quantity is capturing."
  },
  {
    "objectID": "posts/22-11-15-trace/index.html#the-trace",
    "href": "posts/22-11-15-trace/index.html#the-trace",
    "title": "Visualizing the Trace",
    "section": "The Trace",
    "text": "The Trace\nLet’s begin by reviewing the definition of the trace. In Wikipedia [1] and Nielsen+Chuang [2], the first definition provided for the trace is the following. Given a square \\(n\\) by \\(n\\) matrix \\(A\\), its trace is the sum of its diagonal elements:\n\\[\\text{tr}(A) = \\sum_{i = 1}^n A_{ii}.\\]\n\n\nExample\nLet \\(A = \\begin{bmatrix} 4 & 3 \\\\ 2 & 5 \\end{bmatrix}\\). Then \\(\\text{tr}(A) = 4 + 5 = 9\\).\n\nThe definition is simple enough, and it’s a quantity that is very simple to compute too. However, if you’re like me this may appear like a confusing way to characterize a matrix. For example, if we’re using the trace directly to characterize a matrix, we can’t distinguish the following two matrices that seem quite different:\n\\[A = \\begin{bmatrix} 4 & 3 \\\\ 2 & 5 \\end{bmatrix} \\;\\;\\; B = \\begin{bmatrix} 4 & -100000 \\\\ 14838429 & 5 \\end{bmatrix}.\\]\nWe’ll see later that the formal trace distance captures something slightly different, but I’d say this is a reasonable concern to have at first glance of this quantity. For now, let’s see if we can learn more about the trace to resolve this confusion.\n\n\nAlternate definition\nI went back to Strang’s textbook where I first learned about linear algebra [3] to see how he introduced the trace. Interestingly, the first place he introduces the trace is in the chapter about eigenvalues. This gives rise to the alternative definition of trace given by the following.\nGiven an \\(n\\) by \\(n\\) matrix \\(A\\), the trace is equal to the sum of the eigenvalues of \\(A\\).\nThis leads to a quick exercise we can try to verify this claim.\n\n\n\nExercise\nCalculate the eigenvalues of \\(A = \\begin{bmatrix} 4 & 3 \\\\ 2 & 5 \\end{bmatrix}\\) and verify that they do sum to 9.\n\nThis definition was a bit more illuminating for me to get intuition about the trace. If you haven’t seen it before, I highly recommend checking out 3Blue1Brown’s video about eigenvectors and eigenvalues, as it does an amazing job of illustrating the significance of these values. In a nutshell, the eigenvector describes the principal directions that the matrix affects a vector by, and the eigenvalue shows how much that vector gets scaled. Given this, we can now see that the trace of the matrix loosely captures the average scaling that a random vector goes through, when multiplied by \\(A\\)."
  },
  {
    "objectID": "posts/22-11-15-trace/index.html#properties",
    "href": "posts/22-11-15-trace/index.html#properties",
    "title": "Visualizing the Trace",
    "section": "Properties",
    "text": "Properties\nAs a well studied value related to matrices, lots of properties are known about the trace. Let’s go over a few common ones here to see if we can glean some intuition from them.\n\nCyclic\n\nA very useful property that I’ve used often for the trace is that it is cyclic. Formally this is described as the following\n\\[\\text{tr}(AB) = \\text{tr}(BA).\\]\nI was a bit surprised that the above equality holds when I first saw this definition, since it is well known in matrix algebra that generally \\(AB \\neq BA\\). Thus, this property captures something that we lose from the pure matrix product, showing that there is still some relation between matrix products that the trace preserves.\nBefore introducing the next property, here’s a quick note on where this property appears often in quantum information. Suppose we are interested in computing the trace of an operator \\(A\\) after it has been projected to some subset of the Hilbert space spanned by a state \\(\\left|\\psi\\right>\\). This is written as \\(\\text{tr}\\left(A \\left|\\psi \\right> \\left< \\psi \\right|\\right)\\). Using the cyclic property, we can rewrite it as\n\\[\\text{tr}\\left(\\left< \\psi \\right|A \\left|\\psi \\right> \\right) = \\left< \\psi \\right|A \\left|\\psi \\right>\\]\nsince the value in the parentheses is now a scalar. The trace now has become simply the expected value of the operator under the state \\(\\left|\\psi\\right>\\).\n\nLinearity\n\nA second property of the trace is that it is linear. That is,\n\\[\\text{tr}(cA + dB) = c\\text{tr}(A) + d\\text{tr}(B).\\]\nThis property is a favorite amongst mathematicians because it allows analyzing a larger composite element by examining smaller simpler pieces.\n\nSimilarity transformation\n\nThe final property I’ll discuss here is called the similarity transformation. Consider a unitary matrix \\(U\\) (meaning \\(U^\\dagger U = I\\). Then if we take the similarity transformation \\(A \\rightarrow UAU^\\dagger\\), we can use the cyclic property to see that\n\\[\\text{tr}(UAU^\\dagger) = \\text{tr}(U^\\dagger UA) = \\text{tr}(A).\\]\nIn words, the similarity transformation of a matrix preserves its trace. A similarity transformation in some ways serves as a ‘’change of basis’’ or rotation operation. It maintains something similar before and after the transformation, and we see with this definition that one of those things is the trace. Again, this property seems like an entrance to learning more about what exactly we are quantifying when calculating the trace."
  },
  {
    "objectID": "posts/22-11-15-trace/index.html#geometric-interpretation-of-trace",
    "href": "posts/22-11-15-trace/index.html#geometric-interpretation-of-trace",
    "title": "Visualizing the Trace",
    "section": "Geometric Interpretation of Trace",
    "text": "Geometric Interpretation of Trace\nWith these properties of the trace in mind, I’ll highlight a few comments I’ve found surrounding how to visualize the trace. For many of these, it’s going to be most helpful to keep in mind the picture of a matrix as a function acting on a vector, and the geometry comes from the transformation the vector goes through. Most of these are ideas I encountered from MathOverflow [5].\n\nOn projection operators\n\nThe most upvoted response was how to understand the trace for projection operators. A projection matrix \\(A\\) satisfies the property that \\(A^2 = A\\). The name comes from its very geometric action of projecting a vector onto a smaller subspace. Once you project onto the smaller subspace, if you project on the same subspace you’ll keep getting the same vector, which is why the above property holds. It turns out that the trace of a projection operator corresponds to the dimensionality of the subspace that it projects onto. This explains why the eigenvalues of a projection operator turn out to have to be either 0 or 1, as each eigenvalue corresponds to an extra dimension to be projected onto, and if the trace is counting the dimensions, the values should naturally be 0 or 1.\nIt turns out that this is a very fundamental picture in much of mathematics, as the answer is even endorsed and ellaborated upon by Terry Tao himself. I’ve worked with many projectors in quantum information, but hadn’t considered this picture before so I hope this adds some intuition for my future learning.\n\nTrace and the inner product\n\nThe inner product between vectors is something many people study during their undergraduate career. I personally like to think that the inner product characterizes how similar two elements of a vector field are. This perspective is especially useful when working with vectors all of unit length, since an inner product of 0 implies that the vectors are orthogonal, while an inner product of 1 implies that the two vectors are identical. Another thing the inner product allows us to do is capture the notion of the “length” of a vector. In undergrad, many including myself are taught to think of vectors as arrows in Euclidean space, but here I quote the answer given by one of my graduate course professors when he asked if anyone knew what a vector is.\n“A vector is an element of a vector space.”\nSounds a bit obvious and also a bit circular, but his point becomes more clear when we observe the definition of a vector space first. A vector space is just a set of objects that satisfy the eight properties that you can read about in the Wikipedia page [6], which basically boils down to “you can add two elements and they’ll stay in the vector space” and “you can scale elements and they will remain in the vector space”. Equipped with this definition, we now see that a vector space doesn’t have to just be arrows, it can be functions, matrices, and many other strange mathematical objects as long as they satisfy those properties.\nI bring this up because given a vector space, you can also ‘equip’ it with an inner product, promoting it to what is cleverly called an inner product space. The inner product should satisfy some nice properties too that I won’t elaborate about here, but with this inner product, you can now discuss orthogonality, lengths, and other metric properties about the vectors in your space. It turns out that the trace is used in an inner product mathematicians have ‘equipped’ the vector space of matrices with.\nFor two matrices \\(A\\) and \\(B\\) in the space of linear operators (matrices) from \\(\\mathbb{C}^n\\) to \\(\\mathbb{C}^m\\), the inner product between \\(A\\) and \\(B\\) is defined as\n\\[\\left< A, B \\right> \\equiv \\text{Tr} A^\\dagger B.\\]\nOne neat thing about the inner product is that it doesn’t rely on what basis we’re using to describe our elements in, it’ll hold no matter what direction we are looking at our space from. With this connection, this solidifies the intuition that the trace is a basis independent property about matrices. This is consistent with our understanding of how the trace relates to the similarity transform, as that mapping can be considered a change of basis.\nThis perspective will help with understanding the trace distance which I want to take a deep dive into in a future post as well.\n\nOn unit shapes (in finite dimensional Euclidean spaces)\n\nAnother interesting perspective was given as the action of the matrix on vectors that form some unit volume shapes. You can think of a something like a square or a cube with volume 1 centered at the origin with vectors pointing to its vertices starting from the origin. Now suppose you multiplied all of those vectors by a matrix \\(A\\), and you calculated the new volume of the shape. It turns out that the new volume is proportional to the trace of \\(A\\), meaning the volume can be described by \\(t \\cdot \\text{tr}(A)\\) for some constant \\(t\\).\nYou can do something similar with a unit ball, but now you’d have infinitely many vectors so we would need to use integration:\n\\[\\text{Tr}(A) = n \\int_{x \\in B} \\left< Ax, x\\right> dm(x).\\]\nThe value we are integrating is the inner product between the original vector and the newly transformed vector, which again, you can think of as quantifying the amount of change that happens. This perspective is also coordinate independent, and turns out to be something you can construct from the relation to eigenvalues. So again, the trace is an operation that quantifies the amount of change an average element goes through.\n\nLie Algebras\n\nFinally, there were many interesting responses relating the trace to properties of the Lie algebras formed by the operators. I’ll admit I’m not very familiar with Lie algebras, so won’t be able to comment on these with much depth. I would like to write a series introducing Lie algebras as a way for me to get more familiar with them, and perhaps revisit these points when that happens."
  },
  {
    "objectID": "posts/22-11-15-trace/index.html#conclusion",
    "href": "posts/22-11-15-trace/index.html#conclusion",
    "title": "Visualizing the Trace",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we explored the trace in some depth and examined a few ways to build some intuition around what it is that the trace captures. I think the full intuition takes some concrete work with actual matrices to fully develop, but I hope these gave some new ways to think about the trace that will help develop that for you moving forward! Thanks so much for reading :)"
  },
  {
    "objectID": "posts/22-11-15-trace/index.html#references",
    "href": "posts/22-11-15-trace/index.html#references",
    "title": "Visualizing the Trace",
    "section": "References",
    "text": "References\n\nhttps://en.wikipedia.org/wiki/Trace_(linear_algebra)#:~:text=the%20inner%20product%3A-,Cyclic%20property,are%20not%20allowed%3A%20in%20general%2C&text=where%20the%20first%20equality%20is,and%20its%20transpose%20are%20equal.\nQuantum Computation and Quantum Information (Nielsen + Chuang) http://mmrc.amss.cas.cn/tlb/201702/W020170224608149940643.pdf\nIntroduction to Linear Algebra (Strang) https://www.amazon.com/Introduction-Linear-Algebra-Gilbert-Strang/dp/0980232716\nEigenvectors and eigenvalues (3Blue1Brown) https://www.youtube.com/watch?v=PFDu9oVAE-g&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=14\nhttps://mathoverflow.net/questions/13526/geometric-interpretation-of-trace\nhttps://en.wikipedia.org/wiki/Vector_space"
  },
  {
    "objectID": "posts/22-11-28-qcqi-ch10/index.html",
    "href": "posts/22-11-28-qcqi-ch10/index.html",
    "title": "QCQI Chapter 10 Solutions",
    "section": "",
    "text": "My solutions to select exercises in chapter 10 of Nielsen+Chuang’s QCQI."
  },
  {
    "objectID": "posts/22-11-28-qcqi-ch10/index.html#section",
    "href": "posts/22-11-28-qcqi-ch10/index.html#section",
    "title": "QCQI Chapter 10 Solutions",
    "section": "10.5",
    "text": "10.5\n\nExercise 10.29: Let \\(\\left|\\psi_1\\right>, \\left|\\psi_2\\right> \\in V_S\\). We consider an arbitrary linear combination of these two vectors\n\\[\\left|\\psi\\right> = a\\left|\\psi_1\\right> + b \\left|\\psi_2\\right>, \\text{ for some } a, b, \\in \\mathbb{C}.\\]\nNow consider the action of some \\(M \\in S\\) on \\(\\left|\\psi\\right>\\).\n\\[M\\left|\\psi\\right> = M (a\\left|\\psi_1\\right> + b \\left|\\psi_2\\right>) = aM\\left|\\psi_1\\right> + b M\\left|\\psi_2\\right> = a\\left|\\psi_1\\right> + b \\left|\\psi_2\\right> = \\left|\\psi\\right>.\\]\nTherefore, \\(\\left|\\psi\\right> \\in V_S\\).\nIf \\(V_S\\) contains any element from an eigenvalue -1 subspace, we could easily construct a state that violates the above property, so the second statement is true.\n\nExercise 10.30:\nSuppose that \\(\\pm iI \\in S\\). Then, by the group operation this implies that \\(-I \\in S\\). By contraposition, this proves the statement.\n\nExercise 10.31: Let \\(S\\) be a subgroup of \\(G_n\\) generated by elements \\(g_1, \\ldots, g_l\\).\n\\((\\Rightarrow)\\). Suppose that all elements in \\(S\\) commute. That is, for any \\(M, N \\in S\\), we have \\(MN = NM\\). Since \\(g_i, g_j \\in S\\), the generators must commute too..\n\\((\\Leftarrow)\\). Suppose that each pair of generators \\(g_i\\) and \\(g_j\\) commute for all \\(i, j\\). Let \\(M, N \\in S\\), meaning they can be written as products \\(M = g_1^{m_1} \\cdots g_l^{m_l}\\) and \\(N = g_1^{n_1} \\cdots g_l^{n_l}\\) for \\(m_i, n_i \\in \\{0, 1\\}\\). Then\n\\[MN = (g_1^{m_1} \\cdots g_l^{m_l})(g_1^{n_1} \\cdots g_l^{n_l}) = (g_1^{n_1} \\cdots g_l^{n_l})(g_1^{m_1} \\cdots g_l^{m_l}) = NM\\]\nsince we can swap elements one at a time until we go from the second term to the third term in the above equation. Therefore all elements of \\(S\\) commute.\n\nExercise 10.32: Can be easily verified directly.\n\nExercise 10.33:\n\\((\\Rightarrow)\\) Suppose that \\(g\\) and \\(g'\\) commute. Since both are elements of the Pauli group, this means that in each index we have either 1) a non-identity Pauli on one generator and identity on the other or 2) the same Pauli on each position. If both generators are built with identities and the same Pauli, they trivially commute and it is not necessary to test. Another possibility is that the total number of indices with non-commuting terms is even, since in a non-commuting index the commutator picks up a phase of -1. To determine this, we represent \\(g = (x_1, \\ldots, x_n, z_1, \\ldots, z_n)\\) and \\(g' = (x_1', \\ldots, x_n', z_1', \\ldots, z_n')\\), where we have used a length \\(2n\\) bit vector to represent indices of the generator that have (or don’t have) an \\(X\\) component with 1 (or 0), and the same in the second half of the matrix for \\(Z\\). For \\(Y\\) we simply set the corresponding index for both terms to 1. Now if the two generators commute, then we should have\n\\[\\sum_{i=1}^n (x_iz_i' + z_ix_i') = 0,\\]\nwhich is exactly what the ‘twisted’ inner product computes between different rows.\n\\((\\Leftarrow)\\) Suppose that for \\(g, g' \\in S\\), we have \\(r(g)\\Lambda r(g')^T = 0\\). This implies that the total number of non-commuting indices of \\(g\\) and \\(g'\\) is even. This implies \\(g\\) and \\(g'\\) commute since each non-commuting index contributes a phase of -1 when taking the commutator, which when taken to an even power gives 1.\n\nExercise 10.34: Let \\(S = \\left< g_1, \\ldots, g_l \\right>\\). Show that \\(-I\\) is not an element of \\(S\\) if and only if \\(g_j^2 = I\\) for all \\(j\\), and \\(g_j \\neq -I\\) for all \\(j\\).\n\\((\\Rightarrow)\\) Suppose \\(-I \\not \\in S\\). Then, since \\(S\\) is a subgroup of the Pauli group, we have \\(g_j^2 = I\\) for all \\(j\\), but \\(g_j \\neq -I\\) for all \\(j\\) by assumption.\n\\((\\Leftarrow)\\) Suppose \\(g_j^2 = I\\) for all \\(j\\), and \\(g_j \\neq -I\\) for all \\(j\\). By the first condition, we know that \\(g_j \\neq \\pm i M\\) for any \\(n\\) Pauli string \\(M\\). By exercise 10.30, we conclude that \\(-I \\not \\in S\\).\n\nExercise 10.35: Let \\(S\\) be a subgroup of \\(G_n\\) such that \\(-I\\) is not an element of \\(S\\).\nSeems trivial? And very similar to 10.34.\n\nExercise 10.36:\n\\[UX_1U^\\dagger = \\begin{bmatrix}1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&0&1 \\\\ 0&0&1&0\\end{bmatrix} \\begin{bmatrix}0&0&1&0 \\\\ 0&0&0&1 \\\\ 1&0&0&0 \\\\ 0&1&0&0\\end{bmatrix} \\begin{bmatrix}1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&0&1 \\\\ 0&0&1&0\\end{bmatrix} = \\begin{bmatrix}0&0&0&1 \\\\ 0&0&1&0 \\\\ 0&1&0&0 \\\\ 1&0&0&0\\end{bmatrix} = X_1X_2\\]\n\\[UX_2U^\\dagger = \\begin{bmatrix}1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&0&1 \\\\ 0&0&1&0\\end{bmatrix} \\begin{bmatrix}0&1&0&0 \\\\ 1&0&0&0 \\\\ 0&0&0&1 \\\\ 0&0&1&0\\end{bmatrix} \\begin{bmatrix}1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&0&1 \\\\ 0&0&1&0\\end{bmatrix} = \\begin{bmatrix}0&1&0&0 \\\\ 1&0&0&0 \\\\ 0&0&0&1 \\\\ 0&0&1&0\\end{bmatrix} = X_2\\]\n\\[UZ_1U^\\dagger = \\begin{bmatrix}1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&0&1 \\\\ 0&0&1&0\\end{bmatrix} \\begin{bmatrix}1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&-1&0 \\\\ 0&0&0&-1\\end{bmatrix} \\begin{bmatrix}1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&0&1 \\\\ 0&0&1&0\\end{bmatrix} = \\begin{bmatrix}1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&-1&0 \\\\ 0&0&0&-1\\end{bmatrix} = Z_1\\]\n\\[UZ_2U^\\dagger = \\begin{bmatrix}1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&0&1 \\\\ 0&0&1&0\\end{bmatrix} \\begin{bmatrix}1&0&0&0 \\\\ 0&-1&0&0 \\\\ 0&0&1&0 \\\\ 0&0&0&-1\\end{bmatrix} \\begin{bmatrix}1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&0&1 \\\\ 0&0&1&0\\end{bmatrix} = \\begin{bmatrix}1&0&0&0 \\\\ 0&-1&0&0 \\\\ 0&0&-1&0 \\\\ 0&0&0&1\\end{bmatrix} = Z_1Z_2\\]\n\nExercise 10.37: \\[UY_1U^\\dagger = iUX_1Z_1U^\\dagger = iUX_1U^\\dagger U Z_1 U^\\dagger = i (X_1 X_2) Z_1 = Y_1 X_2.\\]\n\nExercise 10.38:\nGot some hints from enakai blog.\nLet \\(M \\in \\{Z_1, Z_2, X_1, X_2\\}\\). By assumption, \\(U\\) and \\(V\\) act on \\(M\\) in the same way under conjugation:\n\\[UMU^\\dagger = VM V^\\dagger \\Leftrightarrow M(U^\\dagger V) = (U^\\dagger V)M.\\]\nNote that we can express any \\(4 \\times 4\\) matrix \\(U^\\dagger V\\) as \\(I_1 \\otimes A + Z_1 \\otimes B + X_1 \\otimes C + Y_1 \\otimes D\\) where \\(A, B, C, D\\) are linear combinations of the 1 qubit Paulis.\nSuppose \\(M = Z_1\\). Consider \\[Z_1(U^\\dagger V) = Z_1(I_1 \\otimes A + Z_1 \\otimes B + X_1 \\otimes C + Y_1 \\otimes D) = Z_1 \\otimes A + I_1 \\otimes B + Z_1(X_1 \\otimes C + Y_1 \\otimes D)\\] and \\[(U^\\dagger V)Z_1 = (I_1 \\otimes A + Z_1 \\otimes B + X_1 \\otimes C + Y_1 \\otimes D)Z_1 = Z_1 \\otimes A + I_1 \\otimes B - Z_1(X_1 \\otimes C + Y_1 \\otimes D).\\]\nSince the two must be equal by assumption, we see that \\(X_1 \\otimes C + Y_1 \\otimes D = 0\\) and \\((U^\\dagger V) = I_1 \\otimes A + Z_1 \\otimes B\\).\nThe same equality must hold for the case \\(M = X_1\\) which we consider next. \\[X_1(U^\\dagger V) = X_1(I_1 \\otimes A + Z_1 \\otimes B)\\] and \\[(U^\\dagger V)X_1 = (I_1 \\otimes A + Z_1 \\otimes B)X_1\\] from which we can see that \\(B = 0\\). This implies that \\(U^\\dagger V = I_1 \\otimes A\\).\nTo determine what \\(A\\) is, we can use a similar analysis with \\(M = Z_2, X_2\\), from which we conclude that \\(U^\\dagger V = I\\), which implies that \\(U = V\\).\n\nExercise 10.39: \\[SXS^\\dagger = \\begin{bmatrix}1&0 \\\\ 0&i\\end{bmatrix} \\begin{bmatrix}0&1 \\\\ 1&0\\end{bmatrix} \\begin{bmatrix}1&0 \\\\ 0&i\\end{bmatrix} = \\begin{bmatrix} 1 & -i \\\\ i & 1 \\end{bmatrix} = Y\\]\n\\[SZS^\\dagger = \\begin{bmatrix}1&0 \\\\ 0&i\\end{bmatrix} \\begin{bmatrix}1&0 \\\\ 0&-1\\end{bmatrix} \\begin{bmatrix}1&0 \\\\ 0&i\\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & =1 \\end{bmatrix} = Z\\]\n\nExercise 10.40:\n\nExercise 10.43: Show that \\(S \\subseteq N(S)\\) for any subgroup \\(S\\) of \\(G_n\\).\n\\[N(S) \\equiv \\{E\\in G_n | EgE^\\dagger \\in S, \\forall g \\in S.\\}\\]\nLet \\(E \\in S\\) where \\(S\\) is a subgroup of \\(N(S)\\). Then for all \\(g \\in S\\), \\(EgE^\\dagger \\in S\\) because \\(S\\) is a subgroup.\n\nExercise 10.44: Show that \\(N(S) = Z(S)\\) for any subgroup \\(S\\) of \\(G_n\\) not containing \\(-I\\).\n(\\(N(S) \\subseteq Z(S)\\)) Take \\(E\\in N(S)\\). Consider the equality \\(EgE = \\pm EE g = \\pm g\\) for some \\(g \\in S\\), where the sign depends on the commutation relation between \\(E\\) and \\(g\\). Suppose there exists an \\(E\\) such that the sign is negative. Since \\(S\\) is a subgroup we have \\(EgE = -g \\in S\\). Then, we find that \\(-I \\in S\\) by\n\\[(EgE) g = -EggE = -I,\\]\nwhich is a contradiction, so we know that \\(EgE = g\\). From this we conclude that \\(Eg = gE\\), so \\(E \\in N(S)\\).\n(\\(Z(S) \\subseteq N(S)\\)) Take \\(E \\in Z(S)\\). Then, \\(Eg = gE\\) for all \\(g \\in S\\). This implies by properties of the Pauli group that \\(EgE^\\dagger = g\\).\n\nExercise 10.45:\n\n**"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Shion Fukuzawa",
    "section": "",
    "text": "Hi! I’m a PhD candidate in computer science at the University of California - Irvine in the Center for Algorithms and Theory of Computation. I am pleased to have Dr. Michael Goodrich and Dr. Sandy Irani as my advisors. My interests are in theoretical computer science, recently focused on quantum computing and quantum information. In my free time, I enjoy playing video games, piano, baking, and taking walks."
  },
  {
    "objectID": "index.html#now",
    "href": "index.html#now",
    "title": "Shion Fukuzawa",
    "section": "Now",
    "text": "Now\nI am teaching CS166 Intro to quantum information and quantum computing. It’s my first time teaching a full course so I am really excited! Check out the course page here.\nResearch wise I have started exploring the class StoqMA more. I am interested to see if there is a search-to-decision reduction in this class, which should exist given the hypothesis that StoqMA = MA."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Shion Fukuzawa",
    "section": "Education",
    "text": "Education\nUniversity of California, Irvine | Irvine, CA | PhD in Computer Science | Sept 2020 - June 2025 (Expected)\nCalvin University | Grand Rapids, MI | B.S in Mathematics | Sept 2016 - May 2020"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Shion Fukuzawa",
    "section": "Publications",
    "text": "Publications\n\nModified Iterative Quantum Amplitude Estimation is Asymptotically Optimal with Chris Ho, Sandy Irani, and Jasen Zion to appear in ALENEX2023\nDiamonds are Forever in the Blockchain: Geometric Polyhedral Point-Set Pattern Matching with Gill Barequet, Michael Goodrich, David Mount, Martha Osegueda, and Evrim Ozel CCCG2022"
  },
  {
    "objectID": "courses/166wi24.html",
    "href": "courses/166wi24.html",
    "title": "Introduction to Quantum Information and Computing",
    "section": "",
    "text": "Announcements\n\n(January 6) Welcome to the course!\n\n\n\nKey Documents and Pages for the Course\n\nMake sure to read through the syllabus. I have outlined the expectations for myself and for you for this quarter amongst other things.\n\n\n\nStaff and Contact Info\nIf you need assistance, please use the Ed discussion board. For personal accommodations you may email me directly.\nInstructor: - Shion Fukuzawa - fukuzaws at uci dot edu - Office hours: MWF 2-3pm @ ICS458C\nTAs: - Fan Wang - Office hours: TBD - Kourosh Mirsohi - Office hours: TBD\n\n\nCourse Outline\nThe following is the breakdown of the three modules we will divide the class into.\n\nModule 1: Math Foundations\n\n[here] Probability and Complex numbers\nLinear algebra\nQuantum circuits\nComplexity\n\nModule 2: Quantum Information\n\nNo cloning theorem and quantum money\nQuantum teleportation\nCHSH game\nHidden variable theory\nError correction\n\nModule 3: Quantum Computing\n\nQuery algorithms: Deutsch-Josza, Bernstein-Vazirani, Simon’s\nQuantum Fourier Transform\nShor’s algorithm\nGrover’s algorithm\nPhase estimation\n\n\n\n\nLecture notes\nThe following are digital copies of the lecture notes. I will post annotated versions as we progress through the course as well.\n\nWeek 1 (unannotated notes, annotated notes)\n\nProbability and complex numbers, linear algebra A, quantum circuits A\n\n\n\n\nProblem Sets\nPlease submit your problem sets to gradescope. If you do not have access to gradescope, let me know ASAP.\n\nNothing here yet…\n\n\n\nExtra practice and references\n\nNothing here yet…\n\n\n\nDiagnostic Exams\n\nNothing here yet…"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "How to Study for (More Than) Your Algorithms Midterm\n\n\n\n\n\n\n\neducation\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nShion Fukuzawa\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArt and AI: A new era for art?\n\n\n\n\n\n\n\ntechnology\n\n\narts\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\nShion Fukuzawa\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQCQI Chapter 10 Solutions\n\n\n\n\n\n\n\nnotes\n\n\nquantum\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2022\n\n\nShion Fukuzawa\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHall LGLAR Chapter 2: The Matrix Exponential\n\n\n\n\n\n\n\nnotes\n\n\nmath\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2022\n\n\nShion Fukuzawa\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHall LGLAR Chapter 3: The Matrix Exponential\n\n\n\n\n\n\n\nnotes\n\n\nmath\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2022\n\n\nShion Fukuzawa\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHall LGLAR Chapter 1: Matrix Lie Groups\n\n\n\n\n\n\n\nnotes\n\n\nmath\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2022\n\n\nShion Fukuzawa\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing the Trace\n\n\n\n\n\n\n\nresearch\n\n\nquantum\n\n\nmath\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nShion Fukuzawa\n\n\n\n\n\n\n  \n\n\n\n\nDiamonds are Forever in the Blockchain\n\n\n\n\n\n\n\nresearch\n\n\ngeometry\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2022\n\n\nShion Fukuzawa\n\n\n\n\n\n\n  \n\n\n\n\nCassava Leaf Detection\n\n\n\n\n\n\n\nprojects\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2021\n\n\nShion Fukuzawa\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiamonds\n\n\n\n\n\n\n\nresearch\n\n\ngeometry\n\n\n\n\n\n\n\n\n\n\n\nJul 15, 2021\n\n\nShion Fukuzawa\n\n\n\n\n\n\nNo matching items"
  }
]