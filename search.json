[
  {
    "objectID": "posts/21-08-14-cassava/index.html",
    "href": "posts/21-08-14-cassava/index.html",
    "title": "Cassava Leaf Detection",
    "section": "",
    "text": "Cassava is one of the largest providers of carbohydrates in Africa, due to its nutritional value and its ability to withstand harsh conditions. However, the crop is not immune to a variety of viral diseases which are the cause of a low crop yield for the 80 percent of sub-Saharan household farms that grow Cassava. Existing methods of detecting viral diseases rely on a small group of experts manually examining each plant to identify the type of disease. This is a very inefficient solution to this problem but works because most viral diseases have clear visually detectable symptoms. Because of this nature, solutions to this problem based on using image data are very well studied as shown in this survey.\nA competition was hosted on Kaggle to come up with techniques to classify Cassava diseases using pictures of the plants. Though we weren’t able to participate in the live version, my group decided this would be a fun problem to tackle as the final project for our machine learning class.\n\n\n\n\n    \n\n\nFigure 1: Example Images from the data set. In the top row, we have bacterial blight, brown streak disease, and green mottle from left to right. On the bottom row, we have mottle disease and a healthy Cassava plant from left to right.\n\n\nSince farmers primarily only have access to mobile devices, we wanted to create a model that was lightweight but still accurate enough to classify photos taken with close to 90% accuracy. To accomplish this, we used convolutional neural networks created through transfer learning of the MobileNetV2 neural network."
  },
  {
    "objectID": "posts/21-08-14-cassava/index.html#convolutional-neural-networks-and-transfer-learning",
    "href": "posts/21-08-14-cassava/index.html#convolutional-neural-networks-and-transfer-learning",
    "title": "Cassava Leaf Detection",
    "section": "Convolutional Neural Networks and Transfer Learning",
    "text": "Convolutional Neural Networks and Transfer Learning\nAfter outperforming all other known models in the 2012 ImageNet Large-Scale Visual Recognition Challenge, deep convolutional neural networks have been seen as a very successful tool in tackling image classification problems. Though powerful, a challenge with CNNs is that they require a huge training dataset and extensive computing power. There are many great resources on CNNs on the web, so I will spare the technical details in this post.\nTo get around this problem, transfer learning has been proposed as a way to take advantage of the quality of CNN models, while not requiring as much resources for training. In transfer learning, a CNN is first pre-trained on a different, much larger data set and later fine-tuned on the data set corresponding to the problem of interest. The transferability of such networks has been an active area of research, and we decided to try it out for this project ourselves.\nSince one of our goals was a lightweight model, we chose MobileNetV2 as our CNN to transfer from. MobileNetV2 is a model developed by a group from Google, that is optimized for mobile devices."
  },
  {
    "objectID": "posts/21-08-14-cassava/index.html#data-augmentation-and-class-balanced-cross-entropy-loss",
    "href": "posts/21-08-14-cassava/index.html#data-augmentation-and-class-balanced-cross-entropy-loss",
    "title": "Cassava Leaf Detection",
    "section": "Data Augmentation and Class-Balanced Cross-Entropy Loss",
    "text": "Data Augmentation and Class-Balanced Cross-Entropy Loss\nWith an architecture in place, we are on a great start, but there is another problem that we are aware of prior to starting the training process. One, we would like to have more training data, and second, the data is heavily skewed towards a single class.\nTo solve the first problem, we use a popular technique in image classification called data augmentation, where we artificially create extra training data by randomly applying a combination of rotations, flips, and scaling operations on the existing data.\nTo solve the second problem, we used the class-balanced cross-entropy loss function that introduces a weighting factor that is inversely proportional to the effective number of samples. The equation to select the effective number of samples is \\(E_{n_i} = (1 - \\beta)^{n_i} / (1 - \\beta)\\) where \\(n_i\\) is the number of samples in class \\(i\\) and \\(\\beta\\) is 0.9999."
  },
  {
    "objectID": "posts/23-01-17-art-ai/index.html",
    "href": "posts/23-01-17-art-ai/index.html",
    "title": "Art and AI",
    "section": "",
    "text": "Earlier this year, OpenAI announced the second version of their text-to-image deep learning model, Dall-E 2. If you haven’t seen it, I highly encourage you to explore the linked page and check out some of what this model is capable of doing. If you’re anything like me, you will be quite shocked and fascinated at the quality and range of the results the model can produce. I remember speaking with fellow academics in computer science who were quite surprised by how soon a model of this caliber appeared. Many of us were expecting something like this to appear in the near future, but not quite so soon (especially considering the level the first Dall-E was at just one year ago).\nOne thing that has always been true throughout our history is that it may take a while for someone to discover something, but once it’s found others are able to learn the mechanics fairly quickly. A similar thing can be said about the technology behind Dall-E 2 (transformers), and we have seen many alternative services deployed throughout this year for generating images from text prompts.\nThough these can be fun to play around with, there have been many debates surrounding the implications and ethics of using this technology. A quick search for AI art on any social media platform will reveal a wide range of opinions on the matter.\nI have some of my own thoughts on the matter, but before sharing those I think it’ll be beneficial to deconstruct the main critique that is brought up against AI art. The following quote from Lois van Baarle’s instagram post captures the essence of the frustrations I’ve seen online: “I wholeheartedly support the ongoing protest against AI art. Why? Because my artwork is included in the datasets used to train these image generators without my consent.” Many of these posts are accompanied by strong anti-AI sentiments and a sense of fear in the comments about the implications of this emerging technology.\nI am on board with regulating the training data that these models have access to in principle, but I also do not think that there is much we are able to do to stop nefarious players to keep training and publicizing models. Because of this, I personally feel that the direction of #noai is unconstructive, and that we should take this opportunity to think about our relationship with art at a deeper level."
  },
  {
    "objectID": "posts/23-01-17-art-ai/index.html#my-thoughts",
    "href": "posts/23-01-17-art-ai/index.html#my-thoughts",
    "title": "Art and AI",
    "section": "My Thoughts",
    "text": "My Thoughts\nAs a huge lover of the arts and a budding computer scientist, this debate battle has been a painful one to watch unfold. However, I am ever hopeful for the future of art and technology especially in light of conversations that are rising now. If I were to summarize my thoughts on the matter, it would be that technologies like this force us to rethink what art means to us, and what we want it to mean to us moving forward.\nA quick stride through the development of computer science will help motivate my point. Computer science studies algorithms, which are just a set of instructions for solving different problems. When creating these algorithms, we are forced to think about small decisions we make subconsciously for different tasks. Talk to any new undergraduate in computer science and they will tell you that it takes a lot of thought to tell a computer how to find a number in a list, something you could ask a child to do with three words: “Where is 27?”.\nIn the process of formalizing this field, an interesting question was posed by Alan Turing. Is it possible to create a machine such that a person wouldn’t be able to distinguish whether it is a human or a machine? This is referred to as the Turing test, and was set up as a major milestone for the field of computer science. The reason this is important, is that many people agreed that when a machine is able to do this, we will be able to call it “intelligent”.\nWe are quite proud of this term we call intelligence, and the advance of technology seems to continually force us to wrestle with what the word means. For example, another “intelligent” pursuit humans have been very proud of is the ability to play chess, at extremely high levels. This is why the victory of Deep Blue against Gary Kasparov was highly controversial, leading to an uproar about the takeover of AI.\nIt’s safe to say the Turing test can also be cleared by many large scale language models these days, as was demonstrated by a Google engineer claiming that an AI chatbot is sentient. It seems that AI is passing the Turing test for image creation too, as is evident by these articles: Reddit’s Art Subreddit Shuts Down After Mod Mistakes Real Artist for AI, A Professional Artist Spent 100 Hours Working On This Book Cover Image, Only To Be Accused Of Using AI.\nWhat is interesting though, is that even though these “feats of intelligence” are now realized by these models, somewhere deep inside we refuse to accept that they’ve become “intelligent”. Chess is still extremely popular (arguably more popular than it’s ever been) and we don’t get excited to share to a chatbot about the crazy day we had. Instead, we realize that these milestones for intelligence didn’t really capture the essence of the word. We push the goal post further, and in this process refine our understanding of intelligence and it’s role in what it means to be human.\nI’ve learned that part of studying computer science is facing a stream of confrontations on our understanding of intelligence. The reason I keep studying it is because the more we learn, my appreciation for the complexity of the human experience increases. I feel a sense of awe similar to when staring into the depths of the ocean, each discovery shedding a ray of light forcing us to confront the reality that the bottom is much deeper than we thought.\nIn a similar way, I feel like we are now being confronted with the opportunity to think about what art is, and the role it plays in our human experience. For me, art is never just about the work itself, but is a bridge for me to connect with others in ways that we can’t in “standard” ways. My experience with arts I love are amplified by the stories it carries both explicitly and implicitly. The world the artist was living in when they created the work and the world I’m experiencing combined generates a unique perception of the work that only I can really experience. Sure, AI can be trained on large databases of images and mix those in clever ways to create new works, but for me it’s hard to imagine that this will replace the role art plays in our lives. I am confident that when we think deeply about what remains after AI rummages through the space of what we call art, we will come out with a deeper appreciation of what it all means to us in the first place.\nAt the end of the day, I may practice against a chess bot, but it’s so that I can play a great game with my friend the next time we’re sitting over a chess board. Chatbots have their useful moments, but our best experiences of them are when they fail and we all laugh at the screenshot together. We won’t lose art if we take the time to slow down enough to share and listen to each others’ stories."
  },
  {
    "objectID": "posts/23-01-17-art-ai/index.html#concluding-remarks",
    "href": "posts/23-01-17-art-ai/index.html#concluding-remarks",
    "title": "Art and AI",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\nI thought about this topic for a while before writing it up, and this is now my current response to any questions surrounding art and AI. Sure it leaves open a lot of questions, like “what is art?”, but I think this is a great opportunity for me to slow down and reevaluate the way I interact with art. When I mindlessly scroll through instagram, am I fully appreciating the humanity and stories of the people who created the work? If I play music while I’m studying or at the gym, is that limiting my focus that should be directed at the musicians? My answer to these questions is no, as is probably the case for many who are reading this. To that end, I want to challenge myself moving forward to wrestle more with this question by taking more time to fully immerse myself in various arts. If AI were to take over art, it’ll be when we stop connecting through the experiences."
  },
  {
    "objectID": "posts/21-07-15-diamonds/index.html",
    "href": "posts/21-07-15-diamonds/index.html",
    "title": "Diamonds",
    "section": "",
    "text": "There is an increasing demand and interest in being able to prove that diamonds are ethically and sustainably sourced. There are several organizations working on creating effective tracking methods using cutting edge technology that combine various tools including blockchain, machine learning, and much more. I heard about this problem from my advisor Michael Goodrich who was wondering if we would be able to come up with effective algorithms to increase the accuracy and effectiveness of this process throughout various stages in the process.\nThere are many interesting questions to be asked about the efficacy and impact having these systems could have, which I might discuss in another post. This post will focus on the question from a purely computer science perspective.\nA diamond goes through several stages before it is sold. After collecting the raw diamonds, these diamonds are sliced into half, then finally carved and polished to look like the jewel you and I are accustomed to seeing. The main role in the tracking systems is to be able to follow a diamond going through this process, and making sure that it is being processed at each stage by organizations committed to ethical and sustainably sourced diamond production.\nTo this end, at each stage of the process, the tracking systems take a snap shot of the diamond in the form of a 3D scan, and uploads it to the database. Having a record of these diamonds should be able to allow users to verify that the diamond didn’t switch midway through the pipeline, which could imply unverified vendors adding their products midway through the system.\nGiven the way current 3D scanning technology works, the results of these scanned objects is not a perfect reconstruction of the object within the computer, but usually a set of points sampled from various parts of the surface object that can approximate the scanned object. This set of points is called a point cloud. Usually, our eyes can process these points and effectively perceive what shape these points form, but it’s not easy to get a computer to understand the point cloud in the way that we do.\nSo now we know that the objects we will be handling in this problem are massive amounts of point clouds of diamonds at various stages. Given these objects, some of the key questions we are interested in exploring are:\n\nGiven two different scans of the same diamond in the same stage, is there an effective way to determine whether they are the same or not?\nGiven two different scans of the same diamond in different stages of the manufacturing process, can we determine which part of the earlier-stage diamond the later-stage diamond came from?\nDiamonds generally have very rigid shapes, even in raw form. Given a scan of points, can we determine what polytope (fancy word for shapes like cubes, tetrahedrons, and other three-dimensional objects with flat sides) it is?\n\nI’ll use this platform to summarize some concepts I learn along the way that might be useful in tackling some of these problems, and hopefully have some updates in the future on any successes!"
  },
  {
    "objectID": "posts/22-08-14-diamonds/index.html",
    "href": "posts/22-08-14-diamonds/index.html",
    "title": "Diamonds are Forever in the Blockchain",
    "section": "",
    "text": "About a year ago, I wrote a blog post about some computational solutions that have been proposed to create a more ethical diamond supply chain. Over the last year, I collaborated with some brilliant people to analyze and propose these solutions. We mainly focused on the process of comparing two different scans of diamonds.\nThroughout the diamond manufacturing process, the diamond is mined, polished, and carved often in different locations before it is finally sold to customers. When the diamond transitions between steps, it is often transferred to a new location. One step of interest is to verify that the correct diamonds have been transferred between each location, to make sure the diamond is properly tracked along the supply chain. From our market research, it appears that organizations like Tracr use machine learning techniques to automate this matching process.\nIn our recent work which can be viewed here, we analyze this problem from the perspective of computational geometry and propose an approximation algorithm to match two scans of diamonds to the desired precision. I’ll give a quick overview of the methods we used, and some of the challenges that we leave open for future exploration.\n\nThe Problem\nThe problem setting we consider is the following: After the necessary processing is completed at location A, we are given a polyhedron C that represents the processed diamond. We are also given a set of points S generated by scanning the same diamond once it arrives at location B. Can you verify that point set S has shape C?\nBelow is a sketch of an example instance of the problem in two dimensions.\nThere’s a body of work that offer solutions to a slight variation of this problem known as point-set registration. In this instance, instead of a polyhedron at location A, we are given another set of points, and we want to compute a translation that aligns these two point sets as best as possible. There are many brilliant algorithms that you can read more about on the linked Wikipedia page and perform very well in practice, but none of these offer theoretical guarantees about performance.\nThere’s also a lot of theoretical work about a specific instance of this problem, where instead of a polyhedron you’re given a circle or a sphere, and you want to find the best fitting one for your set of points. However, there has surprisingly not been much work when considering a polyhedron and a set of points. It turns out it is very challenging to handle the degrees of freedom present in the set of points as well as the possible polyhedra that could be given to you. In this work, we simplified the problem to just look at convex polyhedra.\n\n\nOur Tools\nI won’t dive into the details of the algorithm here, but introduce a few tools we used that I found to be very interesting and fun to work with. Polygons and polyhedrons can be defined in many different ways mathematically, but the way we decided to represent them is using what’s called the polyhedral distance function. (The problem is of primary interest in three dimensions, but we also provide a two-dimensional solution and so most figures will use two dimensions. These ideas aren’t hard to generalize to higher dimensions but are much clearer in two dimensions when drawing them. As such, when I use ‘polyhedron’ in the below explanation, this is interchangeable with ‘polygon’ for the two-dimensional case.)\n\n\nPolyhedral Distance Function\n\nIn simple terms, this distance function has a reference “unit ball” of the polyhedron of interest with a center. It then measures the distance between points p and q by first placing the unit ball centered at p, then returning how much you have to scale the unit ball to touch the point q. This distance function doesn’t necessarily have all the “nice” properties mathematicians want in their distance functions, but it works great in a computational setting.\nWith our mathematical definition of the polyhedron in hand, our algorithm computes the C-shaped minimum width annulus (MWA) of the point set S. Our reasoning for this is that if the two scans indeed represent the same diamond, the minimum width annulus should have a width very close to zero, thus providing a good metric for determining the similarity between the two objects.\n\n\nMinimum Width Annulus\n\nThe MWA is defined as the concentric placement of two copies of the polyhedron such that all the points in the scan lie in between the two scans, such that the difference in polyhedral distance from the center to the two placements is minimized.\n\n\nFuture Directions\nIn our paper, we introduce approximation algorithms to solve this problem in a few different settings (eg. is rotation required?) in two and three dimensions. We do this first by noticing that the search region in the middle can be narrowed down systematically to yield an efficient algorithm, even for high levels of precision. The rotation case turns out to be slightly more demanding, and we imagine there are ways to improve our method in this domain.\nAn interesting direction that can also be explored is the notion of Voronoi diagrams for convex polyhedral distance functions. It turns out the standard Voronoi diagram represents the exact solution to this problem in the case that the shape given is a circle or sphere. We imagine a similar property holds for polygons and polyhedra, but there is a lot we don’t quite understand about Voronoi diagrams for these distance functions."
  },
  {
    "objectID": "posts/22-11-26-lglar-ch2/index.html",
    "href": "posts/22-11-26-lglar-ch2/index.html",
    "title": "Hall LGLAR Chapter 2: The Matrix Exponential",
    "section": "",
    "text": "This is a collection of my notes from chapter 2 of Brian Hall’s “Lie Groups, Lie Algebras, and Representations”. From what I know, the matrix exponential is an important operation used to connect Lie groups and Lie algebras. This chapter introduces the matrix exponential and explores properties about it that should lead to insights on the nature of this connection."
  },
  {
    "objectID": "posts/22-11-26-lglar-ch2/index.html#the-exponential-of-a-matrix",
    "href": "posts/22-11-26-lglar-ch2/index.html#the-exponential-of-a-matrix",
    "title": "Hall LGLAR Chapter 2: The Matrix Exponential",
    "section": "The Exponential of a Matrix",
    "text": "The Exponential of a Matrix\nBefore introducing the exponential of a matrix, we begin by defining the Hilbert-Schmidt norm of a matrix. There are multiple norms that can be defined on the space of matrices, but this seems to be the choice for the study of Lie algebras.\nDefinition 1: For any matrix \\(X \\in M_n(\\mathbb{C})\\), we define\n\\[||X|| = \\left( \\sum_{j,k=1}^n |X_{j,k}|^2\\right)^{1/2}.\\]\nThe quantity \\(||X||\\) is called the Hilbert-Schmidt norm of \\(X\\).\nThis definition is basically the definition of a norm of a vector, where we take the standard vector norm after flattening the matrix \\(X\\), making it quite easy to memorize. A really neat property of the Hilbert-Schmidt norm is that it is independent of the basis, and can be computed by the following:\n\\[||X|| = \\left(\\text{tr} (X^*X)\\right)^{1/2}.\\]\n\nNote: This alternative characterization for the Hilbert-Schmidt norm was somewhat confusing for me at first, but I found a nice way for me to visualize the quantity that the norm is capturing. If you recall, the standard inner product \\(\\left< \\cdot, \\cdot \\right>\\) between vectors \\(x, y \\in \\mathbb{C}^n\\) has the following property when combined with an \\(n \\times n\\) matrix \\(A\\):\n\\[\\left<x, Ay\\right> = \\left< A^* x, y \\right>.\\]\nNow if we ask how much the inner product between two unit vectors \\(\\left< x, y\\right>\\) changes after transforming them by a matrix \\(A\\), we get the following expression:\n\\[\\left<Ax, Ay\\right> = \\left<A^*Ax, y\\right>.\\]\nIn a previous blog post, I explored the trace in an effort to find a geometric interpretation for it. One definition I found was that the trace of an operator characterizes how much an operator scales the volume of a unit ball. Under this picture, the average difference between \\(\\left< x, y\\right>\\) and \\(\\left<Ax, Ay\\right>\\), with the two unit vectors \\(x, y\\) sampled uniformly, is upper-bounded by an expression involving the trace of \\(A^*A\\).\n\\[\\int_x \\int_y \\left<x,y\\right> - \\left<Ax, Ay\\right> dx dy = \\int \\int \\left<x,y\\right> - \\left<A^*Ax, y\\right> dx dy = \\int \\int \\left<x,y\\right> - \\text{tr}(A^*A)/n dx dy \\leq 1 - \\text{tr}(A^*A)/n \\]\n\nThe Hilbert-Schmidt norm satisfies the triangle inequality and a consequence of the Cauchy-Schwartz inequality, expressed as the following:\n\n\\(||X + Y|| \\leq ||X|| + ||Y||\\)\n\\(||XY|| \\leq ||X||\\; ||Y||\\)\n\nWe also take a quick trip back to calculus class to refresh on the definition of a matrix exponential.\nDefinition 2: If \\(X\\) is an \\(n \\times n\\) matrix, the exponential of \\(X\\), written \\(e^X\\), is defined by the usual power series\n\\[e^X = \\sum_{m=0}^\\infty \\frac{X^m}{m!}.\\]\nThe text proves using the Hilbert-Schmidt norm that the above series converges for all \\(X \\in M_n(\\mathbb{C})\\), and proves that \\(e^X\\) is a continuous function of \\(X\\).\nHere we list several properties of the matrix exponential.\nProposition 2.3 (Properties of Matrix Exponential): 1. \\(e^0 = I\\). 2. \\((e^X)^* = e^{X^*}\\). 3. \\(e^X\\) is invertible and \\(\\left(e^X\\right)^{-1} = e^{-X}\\). 4. \\(e^{(\\alpha+\\beta)X} = e^{\\alpha X}e^{\\beta X}\\) for all \\(\\alpha, \\beta \\in \\mathbb{C}\\). 5. If \\(XY = YX\\), then \\(e^{X+Y} = e^Xe^Y = e^Ye^X\\). 6. If \\(C\\) is in \\(\\text{GL}(N;\\mathbb{C})\\), then \\(e^{CXC^{-1}} = Ce^XC^{-1}\\).\nProposition 2.4: Let \\(X\\) be a \\(n \\times n\\) complex matrix. Then \\(e^{tX}\\) is a smooth curve in \\(M_n(\\mathbb{C})\\) and\n\\[\\frac{d}{dt} e^{tX} = X e^{tX} = e^{tX} X.\\]\nIn particular,\n\\[\\frac{d}{dt} e^{tX} \\Big|_{t=0} = X.\\]"
  },
  {
    "objectID": "posts/22-11-26-lglar-ch2/index.html#computing-the-exponential",
    "href": "posts/22-11-26-lglar-ch2/index.html#computing-the-exponential",
    "title": "Hall LGLAR Chapter 2: The Matrix Exponential",
    "section": "Computing the Exponential",
    "text": "Computing the Exponential\nWe can now examine specific examples on how we might compute the exponential of a general matrix. Suppose that we have \\(X \\in M_n(\\mathbb{C})\\) with \\(n\\) linearly independent eigenvectors \\(v_1, \\ldots, v_n\\) with eigenvalues \\(\\lambda_1, \\ldots, \\lambda_n\\). Then the matrix can be diagonalized as\n\\[X = CDC^{-1}\\]\nwhere \\(C\\) is the \\(n \\times n\\) matrix with the eigenvectors of \\(X\\) as its columns, and \\(D\\) is the diagonal \\(n \\times n\\) matrix with the eigenvalues on the diagonal. It is easy to show that the matrix \\(e^D\\) is the matrix with diagonal elements \\(e^{\\lambda_1}, \\ldots, e^{\\lambda_n}\\), so by using property 6 from above, we have that\n\\[e^X = C\\begin{pmatrix} e^{\\lambda_1} & & 0 \\\\ & \\ddots & \\\\ 0 & & e^{\\lambda_n} \\end{pmatrix} C^{-1}.\\]\nSome readers may have encountered the matrix exponential when studying differential equations. Consider the following first-order differential equation\n\\[\\frac{d\\mathbf{v}}{dt} = X\\mathbf{v},\\] \\[\\mathbf{v}(0) = \\mathbf{v}_0,\\]\nwhere \\(\\mathbf{v}(t) \\in \\mathbb{R}^n\\) and \\(X\\) is a fixed \\(n \\times n\\) matrix. The solution of this equation is given by\n\\[\\mathbf{v}(t) = e^{tX}\\mathbf{v}_0,\\]\nwhich can be verified via proposition 2.4."
  },
  {
    "objectID": "posts/22-11-26-lglar-ch2/index.html#the-matrix-logarithm",
    "href": "posts/22-11-26-lglar-ch2/index.html#the-matrix-logarithm",
    "title": "Hall LGLAR Chapter 2: The Matrix Exponential",
    "section": "The Matrix Logarithm",
    "text": "The Matrix Logarithm\nWe also want to define an inverse function to the matrix exponential, which will be called the matrix logarithm. The text describes how this can be done by using techniques and knowledge about the logarithm of complex numbers. I’ll just state the resulting definition here.\nDefinition 3: For an \\(n \\times n\\) matrix \\(A\\), define \\(\\log A\\) by\n\\[\\log A = \\sum_{m=1}^\\infty (-1)^{m+1} \\frac{(A-I)^m}{m}\\]\nwhenever the series converges.\nLike the situation with complex numbers, this function cannot be defined for general matrices, as is formalized in the following theorem.\nTheorem 2.8. The function \\(\\log A\\) is defined and continuous on the set of all \\(n \\times n\\) complex matrices \\(A\\) with \\(||A - I|| < 1\\). For all \\(A \\in M_n(\\mathbb{C})\\) with \\(||A - I|| < 1,\\)\n\\[e^{\\log A} = A.\\]\nFor all \\(X \\in M_n(\\mathbb{C})\\) with \\(||X|| < \\log 2\\), \\(||e^X - I|| < 1\\) and\n\\[\\log e^X = X.\\]\nFinally, the following proposition gives us a way to upperbound the matrix logarithm by a function of the matrix’ Hilbert-Schmidt norm.\nProposition 2.9. There exists a constant \\(c\\) such that for all \\(n \\times n\\) matrices \\(B\\) with \\(||B|| < 1/2\\), we have\n\\[||\\log(I + B) - B|| \\leq c ||B||^2.\\]\nThis can be restated in a more concise way by\n\\[\\log(I + B) = B + O(||B||^2).\\]"
  },
  {
    "objectID": "posts/22-11-26-lglar-ch2/index.html#further-properties-of-the-exponential",
    "href": "posts/22-11-26-lglar-ch2/index.html#further-properties-of-the-exponential",
    "title": "Hall LGLAR Chapter 2: The Matrix Exponential",
    "section": "Further Properties of the Exponential",
    "text": "Further Properties of the Exponential\nThis section introduces several key properties of the matrix exponential that will lead into the discussion of Lie algebras. The first is a familiar equation for those who have seen the Trotter product formula for quantum simulation algorithms.\nTheorem 2.11 (Lie Product Formula). For all \\(X, Y \\in M_n(\\mathbb{C})\\), we have \\[e^{X + Y} = \\underset{m \\rightarrow \\infty}{\\lim} \\left( e^{X/m} e^{Y/m}\\right)^m.\\]\nTheorem 2.12. For any \\(X \\in M_n(\\mathbb{C})\\), we have \\[\\det\\left(e^X\\right) = e^{\\text{trace}(X)}.\\]\nProposition 2.16. The exponential map is an infinitely differentiable map of \\(M_n(\\mathbb{C})\\) into \\(M_n(\\mathbb{C})\\)."
  },
  {
    "objectID": "posts/22-11-26-lglar-ch2/index.html#polar-decomposition",
    "href": "posts/22-11-26-lglar-ch2/index.html#polar-decomposition",
    "title": "Hall LGLAR Chapter 2: The Matrix Exponential",
    "section": "Polar Decomposition",
    "text": "Polar Decomposition\nFinally, I summarize some quick notes on the polar decomposition of a matrix. The introduction to the topic as a generalization of polar decompositions for complex numbers was very interesting, so I suggest taking a look for people who want a better understanding of the topic.\nTheorem 2.17. 1. Every \\(A \\in GL(n;\\mathbb{C})\\) can be written uniquely in the form \\[A = UP\\] where \\(U\\) is unitary and \\(P\\) is self-adjoint and positive. 2. Every self-adjoint positive matrix \\(P\\) can be written uniquely in the form \\[P = e^X\\] with \\(X\\) self-adjoint. Conversely, if \\(X\\) is self-adjoint, then \\(e^X\\) is self-adjoint and positive. 3. If we decompose each \\(A \\in GL(n;\\mathbb{C})\\) uniquely as \\[A = Ue^X\\] with \\(U\\) unitary and \\(X\\) self-adjoint, then \\(U\\) and \\(X\\) depend continuously on \\(A\\)."
  },
  {
    "objectID": "posts/22-11-24-lglar-ch1/index.html",
    "href": "posts/22-11-24-lglar-ch1/index.html",
    "title": "Hall LGLAR Chapter 1: Matrix Lie Groups",
    "section": "",
    "text": "Collection of some notes and solutions to exercises from Brian Hall’s “Lie Groups, Lie Algebras, and Representations”. My primary motivation for going through this literature around Lie groups is to understand the unitary groups and special unitary groups better, so the problems I solve will be focused on those groups more than others. The definition numberings are assigned by myself to reference in the exercise solutions."
  },
  {
    "objectID": "posts/22-11-24-lglar-ch1/index.html#definitions",
    "href": "posts/22-11-24-lglar-ch1/index.html#definitions",
    "title": "Hall LGLAR Chapter 1: Matrix Lie Groups",
    "section": "Definitions",
    "text": "Definitions\nDefinition 1: The general linear group over \\(\\mathbb{R}\\), \\(\\text{GL}(n; \\mathbb{R})\\), is the group of all \\(n\\) by \\(n\\) invertible matrices with real entries, where the group operation is the standard matrix product. A similar definition is used to define \\(\\text{GL}(n; \\mathbb{C})\\).\nDefinition 2: \\(M_n(\\mathbb{C})\\) is the space of all \\(n\\) by \\(n\\) matrices with complex entries.\nDefinition 3: Let \\(A_m\\) be a sequence of complex matrices in \\(M_n(\\mathbb{C})\\). We say that \\(A_m\\) converges to a matrix \\(A\\) if each entry of \\(A_m\\) converges to its corresponding entry in \\(A\\).\nDefinition 4: A matrix Lie group is a subgroup \\(G\\) of \\(\\text{GL}(n;\\mathbb{C})\\) with one of the following properties: 1. If \\(A_m\\) is any sequence of matrices in \\(G\\), and \\(A_m\\) converges to some matrix \\(A\\), then either 1. \\(A \\in G\\) 2. \\(A\\) is not invertible. 2. \\(G\\) is a closed subgroup of \\(\\text{GL}(n;\\mathbb{C})\\).\nDefinition 5: A matrix \\(A\\) is unitary if 2 column vectors are orthonormal, i.e. \\(\\sum_{l=1}^n \\overline{A}_{lj}A_{lk}=\\delta_{jk}\\). Equivalently, a unitary matrix is one that satisfies \\(A^* A = AA^* = I\\)."
  },
  {
    "objectID": "posts/22-11-24-lglar-ch1/index.html#examples-of-matrix-lie-groups",
    "href": "posts/22-11-24-lglar-ch1/index.html#examples-of-matrix-lie-groups",
    "title": "Hall LGLAR Chapter 1: Matrix Lie Groups",
    "section": "Examples of Matrix Lie Groups",
    "text": "Examples of Matrix Lie Groups\n1. Unitary group: The collection of unitary matrices forms a subgroup of \\(\\text{GL}(n;\\mathbb{C})\\) called the unitary group, denoted by \\(U(n)\\). We can also define the special unitary group, \\(SU(n)\\), as the subgroup of \\(U(n)\\) with all matrices having determinant 1.\n\nQuick check 1: Show that \\(G = U(n)\\) (or \\(G = SU(n)\\)) is a matrix Lie group.\nPROOF: To show this, it suffices to show that \\(G\\) satisfies one of the properties in definition 4. From the text, it seems like it is a trivial exercise to show condition 2 holds, so I prove that here (although I’m not sure if this is the cleanest way to prove it).\nThe goal is to show that \\(G\\) is a closed subgroup of \\(\\text{GL}(n;\\mathbb{C})\\). This means that \\(G\\) must be closed as a group, as well as a topological subspace. First to show it is a closed group, we must show \\(U_1U_2 \\in G\\) and \\(U^{-1} \\in G\\) for all \\(U, U_1, U_2 \\in G\\).\n\nSince \\((U_1U_2)^*(U_1U_2) = U_2^* U_1^* U_1 U_2 = I\\), \\(U_1U_2 \\in G\\).\nFor \\(U \\in G\\), \\((UU^{-1})^* = I^* = I\\). By standard matrix identities, \\((UU^{-1})^* = (U^{-1})^*U^* = I\\), implying that \\((U^{-1})^* = (U^*)^{-1}\\). Using this, we have that \\((U^{-1})^* U^{-1} = (U^*)^{-1} U^{-1} = (UU^*)^{-1} = I\\), proving that \\(U^{-1} \\in G\\).\n\nTo show that \\(G\\) is a topologically closed set, first define the function \\(f: U(n) \\rightarrow U(n)\\) as \\(f(A) = A^* A\\). This can be shown to be a continuous function. Furthermore, we can define \\(U(n)\\) as the preimage of \\(\\{I_n\\}\\), a closed set, under this continuous function, showing that \\(U(n)\\) is indeed a closed topological set. To see that this holds for \\(SU(n)\\), a similar proof can be used to show that \\(SL(n\\; \\mathbb{C})\\) is a closed set. Since the intersection of two closed sets is closed, we get \\(SL(n; \\mathbb{C}) \\cap U(n) = SU(n)\\) is closed too.\n\n2. Symplectic group: Consider the skew-symmetric bilinear form \\(B\\) on \\(\\mathbb{R}^{2n}\\) defined by the following:\n\\[\\omega(x, y) = \\sum_{j=1}^n (x_jy_{n+j} - x_{n+j}y_j).\\]\nThe set of all \\(2n \\times 2n\\) matrices \\(A\\) which preserve \\(\\omega\\) is the real symplectic group \\(\\text{Sp}(n;\\mathbb{R})\\), and it is a closed subgroup of \\(\\text{GL}(2n;\\mathbb{R})\\). If \\(\\Omega\\) is the \\(2n \\times 2n\\) matrix\n\\[\\Omega = \\begin{pmatrix}0 & I \\\\ I & 0\\end{pmatrix},\\]\nthen\n\\[\\omega(x,y) = \\left< x, \\Omega y \\right>.\\]\nFrom this, we can show that a \\(2n \\times 2n\\) real matrix \\(A\\) belongs to \\(\\text{Sp}(n;\\mathbb{R})\\) if and only if\n\\[-\\Omega A^{T} \\Omega = A^{-1}.\\]"
  },
  {
    "objectID": "posts/22-11-24-lglar-ch1/index.html#topological-properties",
    "href": "posts/22-11-24-lglar-ch1/index.html#topological-properties",
    "title": "Hall LGLAR Chapter 1: Matrix Lie Groups",
    "section": "Topological Properties",
    "text": "Topological Properties\nThere are three main topological properties we are interested in that a matrix Lie group can satisfy, which are stated in this section.\n\n1. Compactness\nDefinition 6: A matrix Lie group \\(G \\subset \\text{GL}(n;\\mathbb{C})\\) is said to be compact if it is compact in the usual sense as a subset of \\(M_n(\\mathbb{C}) \\cong \\mathbb{R}^{2n^2}\\).\nThe ``usual sense’’ of compactness is hard to get a sense of for those with limited exposure to topology, but thankfully the following theorem provides necessary and sufficient conditions for this property to hold, at least for subsets of Euclidean spaces.\nHeine-Borel Theorem: For a subset \\(S\\) of Euclidean space \\(\\mathbb{R}^n\\), the following two statements are equivalent:\n\n\\(S\\) is closed and bounded\n\\(S\\) is compact, that is, every open cover of S has a finite subcover.\n\nHere, closed means that for any sequence \\(A_m \\in S\\) such that \\(A_m \\rightarrow A\\), then \\(A \\in S\\). We say a subset \\(G \\subset M_n(\\mathbb{C})\\) is bounded if there exists a constant \\(C\\) such that for all \\(A \\in G\\), we have \\(|A_{jk}| \\leq C\\) for all \\(1 \\leq j,k \\leq n\\).\n\nQuick check 2: Show that \\(U(n)\\) and \\(SU(n)\\) are compact.\nPROOF: From quick check 1, we know that \\(U(n)\\) and \\(SU(n)\\) are closed. They are also bounded, since by definition, the columns of matrices in these groups must be unit vectors. This means that \\(|A_{jk}| \\leq 1\\) for all \\(1 \\leq j, k \\leq n\\). By the Heine-Borel theorem, we concluded that these two groups are indeed compact.\n\n\n\n2. Connectedness\nDefinition 7: A matrix Lie group \\(G\\) is path connected if for all \\(A\\) and \\(B\\) in \\(G\\), there exists a continuous path \\(A(t)\\), \\(a \\leq t \\leq b\\), lying in \\(G\\) with \\(A(a) = A\\) and \\(A(b) = B\\). For any matrix Lie group \\(G\\), the identity component of \\(G\\), denoted by \\(G_0\\), is the set of \\(A \\in G\\) for which there exists a continuous path \\(A(t)\\), \\(a \\leq t \\leq b\\), lying in \\(G\\) with \\(A(a) = I\\) and \\(A(b) = A\\).\nIt was stated without proof that for the case of matrix Lie groups, path connectedness implies connectedness, so the two properties are used interchangeably.\n\nQuick check 3: Show that \\(U(n)\\) and \\(SU(n)\\) are connected for all \\(n \\geq 1\\).\nPROOF: Every unitary matrix \\(U\\) has the property that it can be diagonalized, i.e., written as \\(U = VDV^{-1}\\) where \\(V \\in U(n)\\) and \\(D\\) is diagonal with diagonal entries \\(e^{i\\theta_1}, \\ldots, e^{i\\theta_n}\\). We can then define\n\\[U(t) = V\\begin{pmatrix} e^{i(1-t)\\theta_1} & & 0 \\\\  & \\ddots &  \\\\ 0 &  & e^{i(1-t)\\theta_n} \\end{pmatrix}V^{-1}, 0\\leq t \\leq 1.\\]\nIt is clear that \\(U(t)\\) stays in \\(U(n)\\) for all \\(t\\), and \\(U(t)\\) connects \\(U\\) to \\(I\\), showing that \\(U(n)\\) is indeed connected.\nWe can do a similar construction for \\(SU(n)\\), but with the modification that the \\(n\\)-th diagonal element is the inverse of the product of the first \\(n - 1\\) elements, ensuring that the determinant is 1.\n\n\n\n3. Simple Connectedness\nDefinition 8: A matrix Lie group is simply connected if it is connected and, in addition, every loop in \\(G\\) can be shrunken continuously to a point in \\(G\\). More precisely, assume \\(G\\) is connected. Then, \\(G\\) is simply connected if for every continuous path \\(A(t)\\), \\(0 \\leq t \\leq 1\\), lying in \\(G\\) and with \\(A(0) = A(1)\\), there exists a continuous function \\(A(s, t)\\), \\(0 \\leq s, t \\leq 1\\), taking values in \\(G\\) and having the following properties: 1. \\(A(s, 0) = A(s, 1)\\) for all \\(s\\) 2. \\(A(0, t) = A(t)\\) 3. \\(A(1, t) = A(1, 0)\\) for all \\(t\\).\nIt turns out that \\(SU(n)\\) is simply connected for all \\(n\\), but the author defers the proof to a later chapter, possibly implying that we don’t have the necessary tools yet to demonstrate this. However, it is not hard to show that \\(SU(2)\\) is simply connected, as this space (which is the space of states a single qubit can take), is isomorphic to the unit sphere, and the above properties can easily be demonstrated."
  },
  {
    "objectID": "posts/22-11-24-lglar-ch1/index.html#exercises",
    "href": "posts/22-11-24-lglar-ch1/index.html#exercises",
    "title": "Hall LGLAR Chapter 1: Matrix Lie Groups",
    "section": "Exercises",
    "text": "Exercises\nA collection of my solutions to select exercises. The plan is to choose the exercises related to \\(U(n)\\) and \\(SU(n)\\).\n\nExercise 3:\nShow that \\(\\text{Sp}(1;\\mathbb{C}) = \\text{SL}(2;\\mathbb{C})\\) and that \\(\\text{Sp}(1) = SU(2)\\).\nDefinition: The compact symplectic group \\(\\text{Sp}(2)\\) is defined as \\(\\text{Sp}(2) \\equiv \\text{Sp}(1;\\mathbb{C}) \\cap U(2)\\).\nPROOF: Let \\(A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} \\in \\text{SL}(2;\\mathbb{C})\\). If suffices to show that \\(-\\Omega A^T \\Omega = A^{-1}\\) for \\(\\Omega\\) defined above. Since \\(\\det(A) = 1\\), we know that \\(A^{-1} = \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}\\).\n\\[-\\Omega A^T \\Omega = \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} a & c \\\\ b & d \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix} = \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}.\\]\nSo \\(\\text{Sp}(1;\\mathbb{C}) = \\text{SL}(2;\\mathbb{C})\\).\n(\\(\\text{Sp}(2) \\subset SU(2)\\)). Let \\(A \\in SU(2)\\). Then we have \\(\\det(A) = 1\\) and \\(A^*A = I\\) so \\(A \\in SU(2)\\).\n(\\(\\text{Sp}(2) \\supset SU(2)\\)). Let \\(A \\in SU(2)\\). Since \\(SU(2) \\subset U(2)\\) holds by definition, it suffices to show that \\(SU(2) \\subset \\text{Sp}(1; \\mathbb{C})\\). This proof is equivalent to the first part of the problem.\n\\(\\Rightarrow\\) \\(\\text{Sp}(1) = SU(2)\\).\n\nExercise 5 (Part 1): Show that if \\(\\alpha\\) and \\(\\beta\\) are arbitrary complex numbers satisfying \\(|\\alpha|^2 + |\\beta|^2 = 1\\), then\n\\[A = \\begin{pmatrix}\\alpha & -\\overline{\\beta} \\\\ \\beta & \\overline{\\alpha}\\end{pmatrix} \\in SU(2).\\]\nPROOF: Let \\(\\alpha\\) and \\(\\beta\\) be complex numbers satisfying \\(|\\alpha|^2 + |\\beta|^2 = 1\\). Use these to construct the matrix \\(A = \\begin{pmatrix}\\alpha & -\\overline{\\beta} \\\\ \\beta & \\overline{\\alpha}\\end{pmatrix}\\). Then, * \\(\\det{A} = |\\alpha|^2 + |\\beta|^2 = 1\\), and * \\(A^* A = \\begin{pmatrix} \\overline{\\alpha} & \\overline{\\beta} \\\\ -\\beta & \\alpha \\end{pmatrix} \\begin{pmatrix}\\alpha & -\\overline{\\beta} \\\\ \\beta & \\overline{\\alpha}\\end{pmatrix} = I\\).\nSo \\(A \\in SU(2)\\).\nExercise 5 (Part 2): Show that every \\(A \\in SU(2)\\) can be expressed in the above form for a unique pair \\((\\alpha, \\beta)\\) satisfying \\(|\\alpha|^2 + |\\beta|^2\\).\nBegin by taking \\(A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\).\nSince \\(\\det A = 1\\) by definition of \\(SU(2)\\). Also by definition of \\(SU(2)\\), we have that \\(A^{-1} = A^*\\). By definition of inverses of \\(2 \\times 2\\) matrices, we have\n\\[A^{-1} = \\frac{1}{\\det{A}} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix},\\]\nand\n\\[A^* = \\begin{pmatrix} \\overline{a} & \\overline{b} \\\\ \\overline{c} & \\overline{d} \\end{pmatrix}.\\]\nThis implies that \\(d = \\overline{a}\\) and \\(b = -\\overline{c}\\).\nFinally, we know that \\(A^* A = I\\), implying that \\(|a|^2 + |c|^2 = 1\\).\nWe conclude that \\(A = \\begin{pmatrix} a & -\\overline{c} \\\\ c & \\overline{a} \\end{pmatrix}\\) for \\(|a|^2 + |c|^2 = 1\\).\n\nExercise 9: Suppose \\(a\\) is an irrational real number. Show that the set \\(E_a\\) of numbers of the form \\(e^{2\\pi i n a}\\) for \\(n \\in \\mathbb{Z}\\) is dense in the unit circle \\(S^1\\).\nPROOF: First we show that for any \\(n_1 \\neq n_2 \\in \\mathbb{Z}\\), \\(e^{2\\pi i n_1 a} \\neq e^{2\\pi i n_2 a}\\). Without loss of generality, assume that \\(n_1 > n_2\\). Suppose that there was a pair of integers such that \\(e^{2\\pi i n_1 a} = e^{2\\pi i n_2 a}\\). Then,\n\\[e^{2\\pi i n_1 a} - e^{2\\pi i n_2 a} = 0\\] \\[\\Rightarrow \\cos(2\\pi n_1 a) + i \\sin(2\\pi n_1 a) - \\cos(2\\pi n_2 a) - i \\sin (2\\pi n_2 a) = 0\\] \\[\\Rightarrow -2\\sin((2\\pi(n_1 + n_2)a)/2) \\sin((2\\pi(n_1 - n_2)a)/n) + i2\\cos((2\\pi(n_1+n_2)a)/2)\\sin((2\\pi(n_1-n_2)a)/2) = 0\\]\nIt’s simple to show that if any of the values in the parentheses of the sin and cos functions above, it contradicts \\(a\\) being irrational. From this, we see that \\(n_1 \\neq n_2 \\Rightarrow e^{2\\pi i n_1 a} \\neq e^{2\\pi i n_2 a}\\).\nNow consider dividing the unit circle \\(S^1\\) into \\(N\\) evenly sized parts, each with length \\(2\\pi / N\\). Since \\(E_a\\) contains as many elements as there are natural numbers, here must be at least one region that contains an infinite number of points. Since \\(E_a\\) is a subgroup of \\(S^1\\) and the group operation is equivalent to a rotation on the circle, we can rotate the bin with infinite elements to each of the other \\(N - 1\\) arcs, while remaining in the group \\(E_a\\). Now choose \\(N\\) to be the inverse of any \\(\\epsilon < 0\\), and we have shown that \\(E_a\\) is dense in \\(S^1\\)."
  },
  {
    "objectID": "posts/22-11-15-trace/index.html",
    "href": "posts/22-11-15-trace/index.html",
    "title": "Visualizing the Trace",
    "section": "",
    "text": "When studying quantum computing and quantum information, one frequently occurring concept is that of the trace of an operator. This is quite natural given the setting, as matrices occur as the fundamental building blocks of the field. Given this, we encounter the trace in ideas including distance measures (trace distance) and classes of operators (trace preserving maps). Operationally this quantity isn’t too difficult to compute as we’ll see soon, but I’ve struggled to grasp the intuition behind why it’s so important. In this post I’ll be exploring some neat definitions I’ve found for this quantity and hopefully in the process we can get a better grasp on what this mathematical quantity is capturing."
  },
  {
    "objectID": "posts/22-11-15-trace/index.html#the-trace",
    "href": "posts/22-11-15-trace/index.html#the-trace",
    "title": "Visualizing the Trace",
    "section": "The Trace",
    "text": "The Trace\nLet’s begin by reviewing the definition of the trace. In Wikipedia [1] and Nielsen+Chuang [2], the first definition provided for the trace is the following. Given a square \\(n\\) by \\(n\\) matrix \\(A\\), its trace is the sum of its diagonal elements:\n\\[\\text{tr}(A) = \\sum_{i = 1}^n A_{ii}.\\]\n\n\nExample\nLet \\(A = \\begin{bmatrix} 4 & 3 \\\\ 2 & 5 \\end{bmatrix}\\). Then \\(\\text{tr}(A) = 4 + 5 = 9\\).\n\nThe definition is simple enough, and it’s a quantity that is very simple to compute too. However, if you’re like me this may appear like a confusing way to characterize a matrix. For example, if we’re using the trace directly to characterize a matrix, we can’t distinguish the following two matrices that seem quite different:\n\\[A = \\begin{bmatrix} 4 & 3 \\\\ 2 & 5 \\end{bmatrix} \\;\\;\\; B = \\begin{bmatrix} 4 & -100000 \\\\ 14838429 & 5 \\end{bmatrix}.\\]\nWe’ll see later that the formal trace distance captures something slightly different, but I’d say this is a reasonable concern to have at first glance of this quantity. For now, let’s see if we can learn more about the trace to resolve this confusion.\n\n\nAlternate definition\nI went back to Strang’s textbook where I first learned about linear algebra [3] to see how he introduced the trace. Interestingly, the first place he introduces the trace is in the chapter about eigenvalues. This gives rise to the alternative definition of trace given by the following.\nGiven an \\(n\\) by \\(n\\) matrix \\(A\\), the trace is equal to the sum of the eigenvalues of \\(A\\).\nThis leads to a quick exercise we can try to verify this claim.\n\n\n\nExercise\nCalculate the eigenvalues of \\(A = \\begin{bmatrix} 4 & 3 \\\\ 2 & 5 \\end{bmatrix}\\) and verify that they do sum to 9.\n\nThis definition was a bit more illuminating for me to get intuition about the trace. If you haven’t seen it before, I highly recommend checking out 3Blue1Brown’s video about eigenvectors and eigenvalues, as it does an amazing job of illustrating the significance of these values. In a nutshell, the eigenvector describes the principal directions that the matrix affects a vector by, and the eigenvalue shows how much that vector gets scaled. Given this, we can now see that the trace of the matrix loosely captures the average scaling that a random vector goes through, when multiplied by \\(A\\)."
  },
  {
    "objectID": "posts/22-11-15-trace/index.html#properties",
    "href": "posts/22-11-15-trace/index.html#properties",
    "title": "Visualizing the Trace",
    "section": "Properties",
    "text": "Properties\nAs a well studied value related to matrices, lots of properties are known about the trace. Let’s go over a few common ones here to see if we can glean some intuition from them.\n\nCyclic\n\nA very useful property that I’ve used often for the trace is that it is cyclic. Formally this is described as the following\n\\[\\text{tr}(AB) = \\text{tr}(BA).\\]\nI was a bit surprised that the above equality holds when I first saw this definition, since it is well known in matrix algebra that generally \\(AB \\neq BA\\). Thus, this property captures something that we lose from the pure matrix product, showing that there is still some relation between matrix products that the trace preserves.\nBefore introducing the next property, here’s a quick note on where this property appears often in quantum information. Suppose we are interested in computing the trace of an operator \\(A\\) after it has been projected to some subset of the Hilbert space spanned by a state \\(\\left|\\psi\\right>\\). This is written as \\(\\text{tr}\\left(A \\left|\\psi \\right> \\left< \\psi \\right|\\right)\\). Using the cyclic property, we can rewrite it as\n\\[\\text{tr}\\left(\\left< \\psi \\right|A \\left|\\psi \\right> \\right) = \\left< \\psi \\right|A \\left|\\psi \\right>\\]\nsince the value in the parentheses is now a scalar. The trace now has become simply the expected value of the operator under the state \\(\\left|\\psi\\right>\\).\n\nLinearity\n\nA second property of the trace is that it is linear. That is,\n\\[\\text{tr}(cA + dB) = c\\text{tr}(A) + d\\text{tr}(B).\\]\nThis property is a favorite amongst mathematicians because it allows analyzing a larger composite element by examining smaller simpler pieces.\n\nSimilarity transformation\n\nThe final property I’ll discuss here is called the similarity transformation. Consider a unitary matrix \\(U\\) (meaning \\(U^\\dagger U = I\\). Then if we take the similarity transformation \\(A \\rightarrow UAU^\\dagger\\), we can use the cyclic property to see that\n\\[\\text{tr}(UAU^\\dagger) = \\text{tr}(U^\\dagger UA) = \\text{tr}(A).\\]\nIn words, the similarity transformation of a matrix preserves its trace. A similarity transformation in some ways serves as a ‘’change of basis’’ or rotation operation. It maintains something similar before and after the transformation, and we see with this definition that one of those things is the trace. Again, this property seems like an entrance to learning more about what exactly we are quantifying when calculating the trace."
  },
  {
    "objectID": "posts/22-11-15-trace/index.html#geometric-interpretation-of-trace",
    "href": "posts/22-11-15-trace/index.html#geometric-interpretation-of-trace",
    "title": "Visualizing the Trace",
    "section": "Geometric Interpretation of Trace",
    "text": "Geometric Interpretation of Trace\nWith these properties of the trace in mind, I’ll highlight a few comments I’ve found surrounding how to visualize the trace. For many of these, it’s going to be most helpful to keep in mind the picture of a matrix as a function acting on a vector, and the geometry comes from the transformation the vector goes through. Most of these are ideas I encountered from MathOverflow [5].\n\nOn projection operators\n\nThe most upvoted response was how to understand the trace for projection operators. A projection matrix \\(A\\) satisfies the property that \\(A^2 = A\\). The name comes from its very geometric action of projecting a vector onto a smaller subspace. Once you project onto the smaller subspace, if you project on the same subspace you’ll keep getting the same vector, which is why the above property holds. It turns out that the trace of a projection operator corresponds to the dimensionality of the subspace that it projects onto. This explains why the eigenvalues of a projection operator turn out to have to be either 0 or 1, as each eigenvalue corresponds to an extra dimension to be projected onto, and if the trace is counting the dimensions, the values should naturally be 0 or 1.\nIt turns out that this is a very fundamental picture in much of mathematics, as the answer is even endorsed and ellaborated upon by Terry Tao himself. I’ve worked with many projectors in quantum information, but hadn’t considered this picture before so I hope this adds some intuition for my future learning.\n\nTrace and the inner product\n\nThe inner product between vectors is something many people study during their undergraduate career. I personally like to think that the inner product characterizes how similar two elements of a vector field are. This perspective is especially useful when working with vectors all of unit length, since an inner product of 0 implies that the vectors are orthogonal, while an inner product of 1 implies that the two vectors are identical. Another thing the inner product allows us to do is capture the notion of the “length” of a vector. In undergrad, many including myself are taught to think of vectors as arrows in Euclidean space, but here I quote the answer given by one of my graduate course professors when he asked if anyone knew what a vector is.\n“A vector is an element of a vector space.”\nSounds a bit obvious and also a bit circular, but his point becomes more clear when we observe the definition of a vector space first. A vector space is just a set of objects that satisfy the eight properties that you can read about in the Wikipedia page [6], which basically boils down to “you can add two elements and they’ll stay in the vector space” and “you can scale elements and they will remain in the vector space”. Equipped with this definition, we now see that a vector space doesn’t have to just be arrows, it can be functions, matrices, and many other strange mathematical objects as long as they satisfy those properties.\nI bring this up because given a vector space, you can also ‘equip’ it with an inner product, promoting it to what is cleverly called an inner product space. The inner product should satisfy some nice properties too that I won’t elaborate about here, but with this inner product, you can now discuss orthogonality, lengths, and other metric properties about the vectors in your space. It turns out that the trace is used in an inner product mathematicians have ‘equipped’ the vector space of matrices with.\nFor two matrices \\(A\\) and \\(B\\) in the space of linear operators (matrices) from \\(\\mathbb{C}^n\\) to \\(\\mathbb{C}^m\\), the inner product between \\(A\\) and \\(B\\) is defined as\n\\[\\left< A, B \\right> \\equiv \\text{Tr} A^\\dagger B.\\]\nOne neat thing about the inner product is that it doesn’t rely on what basis we’re using to describe our elements in, it’ll hold no matter what direction we are looking at our space from. With this connection, this solidifies the intuition that the trace is a basis independent property about matrices. This is consistent with our understanding of how the trace relates to the similarity transform, as that mapping can be considered a change of basis.\nThis perspective will help with understanding the trace distance which I want to take a deep dive into in a future post as well.\n\nOn unit shapes (in finite dimensional Euclidean spaces)\n\nAnother interesting perspective was given as the action of the matrix on vectors that form some unit volume shapes. You can think of a something like a square or a cube with volume 1 centered at the origin with vectors pointing to its vertices starting from the origin. Now suppose you multiplied all of those vectors by a matrix \\(A\\), and you calculated the new volume of the shape. It turns out that the new volume is proportional to the trace of \\(A\\), meaning the volume can be described by \\(t \\cdot \\text{tr}(A)\\) for some constant \\(t\\).\nYou can do something similar with a unit ball, but now you’d have infinitely many vectors so we would need to use integration:\n\\[\\text{Tr}(A) = n \\int_{x \\in B} \\left< Ax, x\\right> dm(x).\\]\nThe value we are integrating is the inner product between the original vector and the newly transformed vector, which again, you can think of as quantifying the amount of change that happens. This perspective is also coordinate independent, and turns out to be something you can construct from the relation to eigenvalues. So again, the trace is an operation that quantifies the amount of change an average element goes through.\n\nLie Algebras\n\nFinally, there were many interesting responses relating the trace to properties of the Lie algebras formed by the operators. I’ll admit I’m not very familiar with Lie algebras, so won’t be able to comment on these with much depth. I would like to write a series introducing Lie algebras as a way for me to get more familiar with them, and perhaps revisit these points when that happens."
  },
  {
    "objectID": "posts/22-11-15-trace/index.html#conclusion",
    "href": "posts/22-11-15-trace/index.html#conclusion",
    "title": "Visualizing the Trace",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we explored the trace in some depth and examined a few ways to build some intuition around what it is that the trace captures. I think the full intuition takes some concrete work with actual matrices to fully develop, but I hope these gave some new ways to think about the trace that will help develop that for you moving forward! Thanks so much for reading :)"
  },
  {
    "objectID": "posts/22-11-15-trace/index.html#references",
    "href": "posts/22-11-15-trace/index.html#references",
    "title": "Visualizing the Trace",
    "section": "References",
    "text": "References\n\nhttps://en.wikipedia.org/wiki/Trace_(linear_algebra)#:~:text=the%20inner%20product%3A-,Cyclic%20property,are%20not%20allowed%3A%20in%20general%2C&text=where%20the%20first%20equality%20is,and%20its%20transpose%20are%20equal.\nQuantum Computation and Quantum Information (Nielsen + Chuang) http://mmrc.amss.cas.cn/tlb/201702/W020170224608149940643.pdf\nIntroduction to Linear Algebra (Strang) https://www.amazon.com/Introduction-Linear-Algebra-Gilbert-Strang/dp/0980232716\nEigenvectors and eigenvalues (3Blue1Brown) https://www.youtube.com/watch?v=PFDu9oVAE-g&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=14\nhttps://mathoverflow.net/questions/13526/geometric-interpretation-of-trace\nhttps://en.wikipedia.org/wiki/Vector_space"
  },
  {
    "objectID": "posts/22-11-28-qcqi-ch10/index.html",
    "href": "posts/22-11-28-qcqi-ch10/index.html",
    "title": "QCQI Chapter 10 Solutions",
    "section": "",
    "text": "My solutions to select exercises in chapter 10 of Nielsen+Chuang’s QCQI."
  },
  {
    "objectID": "posts/22-11-28-qcqi-ch10/index.html#section",
    "href": "posts/22-11-28-qcqi-ch10/index.html#section",
    "title": "QCQI Chapter 10 Solutions",
    "section": "10.5",
    "text": "10.5\n\nExercise 10.29: Let \\(\\left|\\psi_1\\right>, \\left|\\psi_2\\right> \\in V_S\\). We consider an arbitrary linear combination of these two vectors\n\\[\\left|\\psi\\right> = a\\left|\\psi_1\\right> + b \\left|\\psi_2\\right>, \\text{ for some } a, b, \\in \\mathbb{C}.\\]\nNow consider the action of some \\(M \\in S\\) on \\(\\left|\\psi\\right>\\).\n\\[M\\left|\\psi\\right> = M (a\\left|\\psi_1\\right> + b \\left|\\psi_2\\right>) = aM\\left|\\psi_1\\right> + b M\\left|\\psi_2\\right> = a\\left|\\psi_1\\right> + b \\left|\\psi_2\\right> = \\left|\\psi\\right>.\\]\nTherefore, \\(\\left|\\psi\\right> \\in V_S\\).\nIf \\(V_S\\) contains any element from an eigenvalue -1 subspace, we could easily construct a state that violates the above property, so the second statement is true.\n\nExercise 10.30:\nSuppose that \\(\\pm iI \\in S\\). Then, by the group operation this implies that \\(-I \\in S\\). By contraposition, this proves the statement.\n\nExercise 10.31: Let \\(S\\) be a subgroup of \\(G_n\\) generated by elements \\(g_1, \\ldots, g_l\\).\n\\((\\Rightarrow)\\). Suppose that all elements in \\(S\\) commute. That is, for any \\(M, N \\in S\\), we have \\(MN = NM\\). Since \\(g_i, g_j \\in S\\), the generators must commute too..\n\\((\\Leftarrow)\\). Suppose that each pair of generators \\(g_i\\) and \\(g_j\\) commute for all \\(i, j\\). Let \\(M, N \\in S\\), meaning they can be written as products \\(M = g_1^{m_1} \\cdots g_l^{m_l}\\) and \\(N = g_1^{n_1} \\cdots g_l^{n_l}\\) for \\(m_i, n_i \\in \\{0, 1\\}\\). Then\n\\[MN = (g_1^{m_1} \\cdots g_l^{m_l})(g_1^{n_1} \\cdots g_l^{n_l}) = (g_1^{n_1} \\cdots g_l^{n_l})(g_1^{m_1} \\cdots g_l^{m_l}) = NM\\]\nsince we can swap elements one at a time until we go from the second term to the third term in the above equation. Therefore all elements of \\(S\\) commute.\n\nExercise 10.32: Can be easily verified directly.\n\nExercise 10.33:\n\\((\\Rightarrow)\\) Suppose that \\(g\\) and \\(g'\\) commute. Since both are elements of the Pauli group, this means that in each index we have either 1) a non-identity Pauli on one generator and identity on the other or 2) the same Pauli on each position. If both generators are built with identities and the same Pauli, they trivially commute and it is not necessary to test. Another possibility is that the total number of indices with non-commuting terms is even, since in a non-commuting index the commutator picks up a phase of -1. To determine this, we represent \\(g = (x_1, \\ldots, x_n, z_1, \\ldots, z_n)\\) and \\(g' = (x_1', \\ldots, x_n', z_1', \\ldots, z_n')\\), where we have used a length \\(2n\\) bit vector to represent indices of the generator that have (or don’t have) an \\(X\\) component with 1 (or 0), and the same in the second half of the matrix for \\(Z\\). For \\(Y\\) we simply set the corresponding index for both terms to 1. Now if the two generators commute, then we should have\n\\[\\sum_{i=1}^n (x_iz_i' + z_ix_i') = 0,\\]\nwhich is exactly what the ‘twisted’ inner product computes between different rows.\n\\((\\Leftarrow)\\) Suppose that for \\(g, g' \\in S\\), we have \\(r(g)\\Lambda r(g')^T = 0\\). This implies that the total number of non-commuting indices of \\(g\\) and \\(g'\\) is even. This implies \\(g\\) and \\(g'\\) commute since each non-commuting index contributes a phase of -1 when taking the commutator, which when taken to an even power gives 1.\n\nExercise 10.34: Let \\(S = \\left< g_1, \\ldots, g_l \\right>\\). Show that \\(-I\\) is not an element of \\(S\\) if and only if \\(g_j^2 = I\\) for all \\(j\\), and \\(g_j \\neq -I\\) for all \\(j\\).\n\\((\\Rightarrow)\\) Suppose \\(-I \\not \\in S\\). Then, since \\(S\\) is a subgroup of the Pauli group, we have \\(g_j^2 = I\\) for all \\(j\\), but \\(g_j \\neq -I\\) for all \\(j\\) by assumption.\n\\((\\Leftarrow)\\) Suppose \\(g_j^2 = I\\) for all \\(j\\), and \\(g_j \\neq -I\\) for all \\(j\\). By the first condition, we know that \\(g_j \\neq \\pm i M\\) for any \\(n\\) Pauli string \\(M\\). By exercise 10.30, we conclude that \\(-I \\not \\in S\\).\n\nExercise 10.35: Let \\(S\\) be a subgroup of \\(G_n\\) such that \\(-I\\) is not an element of \\(S\\).\nSeems trivial? And very similar to 10.34.\n\nExercise 10.36:\n\\[UX_1U^\\dagger = \\begin{bmatrix}1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&0&1 \\\\ 0&0&1&0\\end{bmatrix} \\begin{bmatrix}0&0&1&0 \\\\ 0&0&0&1 \\\\ 1&0&0&0 \\\\ 0&1&0&0\\end{bmatrix} \\begin{bmatrix}1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&0&1 \\\\ 0&0&1&0\\end{bmatrix} = \\begin{bmatrix}0&0&0&1 \\\\ 0&0&1&0 \\\\ 0&1&0&0 \\\\ 1&0&0&0\\end{bmatrix} = X_1X_2\\]\n\\[UX_2U^\\dagger = \\begin{bmatrix}1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&0&1 \\\\ 0&0&1&0\\end{bmatrix} \\begin{bmatrix}0&1&0&0 \\\\ 1&0&0&0 \\\\ 0&0&0&1 \\\\ 0&0&1&0\\end{bmatrix} \\begin{bmatrix}1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&0&1 \\\\ 0&0&1&0\\end{bmatrix} = \\begin{bmatrix}0&1&0&0 \\\\ 1&0&0&0 \\\\ 0&0&0&1 \\\\ 0&0&1&0\\end{bmatrix} = X_2\\]\n\\[UZ_1U^\\dagger = \\begin{bmatrix}1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&0&1 \\\\ 0&0&1&0\\end{bmatrix} \\begin{bmatrix}1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&-1&0 \\\\ 0&0&0&-1\\end{bmatrix} \\begin{bmatrix}1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&0&1 \\\\ 0&0&1&0\\end{bmatrix} = \\begin{bmatrix}1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&-1&0 \\\\ 0&0&0&-1\\end{bmatrix} = Z_1\\]\n\\[UZ_2U^\\dagger = \\begin{bmatrix}1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&0&1 \\\\ 0&0&1&0\\end{bmatrix} \\begin{bmatrix}1&0&0&0 \\\\ 0&-1&0&0 \\\\ 0&0&1&0 \\\\ 0&0&0&-1\\end{bmatrix} \\begin{bmatrix}1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&0&1 \\\\ 0&0&1&0\\end{bmatrix} = \\begin{bmatrix}1&0&0&0 \\\\ 0&-1&0&0 \\\\ 0&0&-1&0 \\\\ 0&0&0&1\\end{bmatrix} = Z_1Z_2\\]\n\nExercise 10.37: \\[UY_1U^\\dagger = iUX_1Z_1U^\\dagger = iUX_1U^\\dagger U Z_1 U^\\dagger = i (X_1 X_2) Z_1 = Y_1 X_2.\\]\n\nExercise 10.38:\nGot some hints from enakai blog.\nLet \\(M \\in \\{Z_1, Z_2, X_1, X_2\\}\\). By assumption, \\(U\\) and \\(V\\) act on \\(M\\) in the same way under conjugation:\n\\[UMU^\\dagger = VM V^\\dagger \\Leftrightarrow M(U^\\dagger V) = (U^\\dagger V)M.\\]\nNote that we can express any \\(4 \\times 4\\) matrix \\(U^\\dagger V\\) as \\(I_1 \\otimes A + Z_1 \\otimes B + X_1 \\otimes C + Y_1 \\otimes D\\) where \\(A, B, C, D\\) are linear combinations of the 1 qubit Paulis.\nSuppose \\(M = Z_1\\). Consider \\[Z_1(U^\\dagger V) = Z_1(I_1 \\otimes A + Z_1 \\otimes B + X_1 \\otimes C + Y_1 \\otimes D) = Z_1 \\otimes A + I_1 \\otimes B + Z_1(X_1 \\otimes C + Y_1 \\otimes D)\\] and \\[(U^\\dagger V)Z_1 = (I_1 \\otimes A + Z_1 \\otimes B + X_1 \\otimes C + Y_1 \\otimes D)Z_1 = Z_1 \\otimes A + I_1 \\otimes B - Z_1(X_1 \\otimes C + Y_1 \\otimes D).\\]\nSince the two must be equal by assumption, we see that \\(X_1 \\otimes C + Y_1 \\otimes D = 0\\) and \\((U^\\dagger V) = I_1 \\otimes A + Z_1 \\otimes B\\).\nThe same equality must hold for the case \\(M = X_1\\) which we consider next. \\[X_1(U^\\dagger V) = X_1(I_1 \\otimes A + Z_1 \\otimes B)\\] and \\[(U^\\dagger V)X_1 = (I_1 \\otimes A + Z_1 \\otimes B)X_1\\] from which we can see that \\(B = 0\\). This implies that \\(U^\\dagger V = I_1 \\otimes A\\).\nTo determine what \\(A\\) is, we can use a similar analysis with \\(M = Z_2, X_2\\), from which we conclude that \\(U^\\dagger V = I\\), which implies that \\(U = V\\).\n\nExercise 10.39: \\[SXS^\\dagger = \\begin{bmatrix}1&0 \\\\ 0&i\\end{bmatrix} \\begin{bmatrix}0&1 \\\\ 1&0\\end{bmatrix} \\begin{bmatrix}1&0 \\\\ 0&i\\end{bmatrix} = \\begin{bmatrix} 1 & -i \\\\ i & 1 \\end{bmatrix} = Y\\]\n\\[SZS^\\dagger = \\begin{bmatrix}1&0 \\\\ 0&i\\end{bmatrix} \\begin{bmatrix}1&0 \\\\ 0&-1\\end{bmatrix} \\begin{bmatrix}1&0 \\\\ 0&i\\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & =1 \\end{bmatrix} = Z\\]\n\nExercise 10.40:\n\nExercise 10.43: Show that \\(S \\subseteq N(S)\\) for any subgroup \\(S\\) of \\(G_n\\).\n\\[N(S) \\equiv \\{E\\in G_n | EgE^\\dagger \\in S, \\forall g \\in S.\\}\\]\nLet \\(E \\in S\\) where \\(S\\) is a subgroup of \\(N(S)\\). Then for all \\(g \\in S\\), \\(EgE^\\dagger \\in S\\) because \\(S\\) is a subgroup.\n\nExercise 10.44: Show that \\(N(S) = Z(S)\\) for any subgroup \\(S\\) of \\(G_n\\) not containing \\(-I\\).\n(\\(N(S) \\subseteq Z(S)\\)) Take \\(E\\in N(S)\\). Consider the equality \\(EgE = \\pm EE g = \\pm g\\) for some \\(g \\in S\\), where the sign depends on the commutation relation between \\(E\\) and \\(g\\). Suppose there exists an \\(E\\) such that the sign is negative. Since \\(S\\) is a subgroup we have \\(EgE = -g \\in S\\). Then, we find that \\(-I \\in S\\) by\n\\[(EgE) g = -EggE = -I,\\]\nwhich is a contradiction, so we know that \\(EgE = g\\). From this we conclude that \\(Eg = gE\\), so \\(E \\in N(S)\\).\n(\\(Z(S) \\subseteq N(S)\\)) Take \\(E \\in Z(S)\\). Then, \\(Eg = gE\\) for all \\(g \\in S\\). This implies by properties of the Pauli group that \\(EgE^\\dagger = g\\).\n\n**"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Shion Fukuzawa",
    "section": "",
    "text": "Hi! I’m a PhD student in computer science at the University of California - Irvine in the Center for Algorithms and Theory of Computation. I am pleased to have Dr. Michael Goodrich and Dr. Sandy Irani as my advisors. My interests are in theoretical computer science, specifically computational geometry and quantum computing. In my free time, I enjoy playing video games, baking, and taking walks."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Shion Fukuzawa",
    "section": "Education",
    "text": "Education\nUniversity of California, Irvine | Irvine, CA | PhD in Computer Science | Sept 2020 - June 2025 (Expected)\nCalvin University | Grand Rapids, MI | B.S in Mathematics | Sept 2016 - May 2020"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Shion Fukuzawa",
    "section": "Publications",
    "text": "Publications\n\nModified Iterative Quantum Amplitude Estimation is Asymptotically Optimal with Chris Ho, Sandy Irani, and Jasen Zion to appear in ALENEX2023\nDiamonds are Forever in the Blockchain: Geometric Polyhedral Point-Set Pattern Matching with Gill Barequet, Michael Goodrich, David Mount, Martha Osegueda, and Evrim Ozel CCCG2022"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Art and AI\n\n\n\n\n\n\n\ntechnology\n\n\narts\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\nShion Fukuzawa\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQCQI Chapter 10 Solutions\n\n\n\n\n\n\n\nnotes\n\n\nquantum\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2022\n\n\nShion Fukuzawa\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHall LGLAR Chapter 2: The Matrix Exponential\n\n\n\n\n\n\n\nnotes\n\n\nmath\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2022\n\n\nShion Fukuzawa\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHall LGLAR Chapter 1: Matrix Lie Groups\n\n\n\n\n\n\n\nnotes\n\n\nmath\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2022\n\n\nShion Fukuzawa\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing the Trace\n\n\n\n\n\n\n\nresearch\n\n\nquantum\n\n\nmath\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nShion Fukuzawa\n\n\n\n\n\n\n  \n\n\n\n\nDiamonds are Forever in the Blockchain\n\n\n\n\n\n\n\nresearch\n\n\ngeometry\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2022\n\n\nShion Fukuzawa\n\n\n\n\n\n\n  \n\n\n\n\nCassava Leaf Detection\n\n\n\n\n\n\n\nprojects\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2021\n\n\nShion Fukuzawa\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiamonds\n\n\n\n\n\n\n\nresearch\n\n\ngeometry\n\n\n\n\n\n\n\n\n\n\n\nJul 15, 2021\n\n\nShion Fukuzawa\n\n\n\n\n\n\nNo matching items"
  }
]